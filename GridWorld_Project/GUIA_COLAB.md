# üöÄ Guia R√°pido: Google Colab

## ‚ö° Setup em 3 Passos (2 minutos)

### Passo 1Ô∏è‚É£: Upload dos Arquivos

Cole esta c√©lula no Colab e execute:

```python
from google.colab import files
uploaded = files.upload()
print(f"‚úÖ {len(uploaded)} arquivo(s) carregado(s)")
```

**üëÜ Clique em "Escolher arquivos" e selecione:**
- `environment.py`
- `algorithms.py`
- `visualization.py`

---

### Passo 2Ô∏è‚É£: Imports

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld, plot_learning_curves

print("‚úÖ Pronto!")
```

---

### Passo 3Ô∏è‚É£: Executar

```python
gw = create_classic_gridworld()
Q, rewards = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)
visualize_gridworld(gw, policy=get_greedy_policy(Q, gw))
plot_learning_curves({'Q-Learning': rewards})
```

**üéâ Pronto! Voc√™ j√° est√° usando RL!**

---

## üìã Template Completo para Colab

Copie e cole este notebook completo:

```python
# =============================================================================
# REINFORCEMENT LEARNING - GRIDWORLD
# Template Completo para Google Colab
# =============================================================================

# -----------------------------------------------------------------------------
# 1Ô∏è‚É£ UPLOAD DOS ARQUIVOS
# -----------------------------------------------------------------------------
from google.colab import files
print("üì§ Fa√ßa upload dos 3 arquivos .py")
uploaded = files.upload()

# -----------------------------------------------------------------------------
# 2Ô∏è‚É£ VERIFICAR ARQUIVOS
# -----------------------------------------------------------------------------
import os
arquivos = ['environment.py', 'algorithms.py', 'visualization.py']
ok = all(f in os.listdir('.') for f in arquivos)
print("‚úÖ Arquivos OK" if ok else "‚ùå Faltam arquivos")

# -----------------------------------------------------------------------------
# 3Ô∏è‚É£ IMPORTS
# -----------------------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

from environment import create_classic_gridworld, create_cliff_world
from algorithms import sarsa, q_learning, expected_sarsa, get_greedy_policy
from visualization import (
    visualize_gridworld, 
    plot_learning_curves, 
    compare_algorithms
)

print("‚úÖ M√≥dulos importados!")

# -----------------------------------------------------------------------------
# 4Ô∏è‚É£ CRIAR AMBIENTE
# -----------------------------------------------------------------------------
gw = create_classic_gridworld()
visualize_gridworld(gw, title="GridWorld 4x3")

# -----------------------------------------------------------------------------
# 5Ô∏è‚É£ TREINAR ALGORITMOS
# -----------------------------------------------------------------------------
print("üß† Treinando...")

Q_q, rewards_q = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)
Q_s, rewards_s = sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)
Q_e, rewards_e = expected_sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)

print("‚úÖ Treinamento conclu√≠do!")

# -----------------------------------------------------------------------------
# 6Ô∏è‚É£ VISUALIZAR RESULTADOS
# -----------------------------------------------------------------------------
print("üìä Visualizando...")

# Curvas de aprendizado
plot_learning_curves({
    'Q-Learning': rewards_q,
    'SARSA': rewards_s,
    'Expected SARSA': rewards_e
})

# Pol√≠ticas
for nome, Q in [('Q-Learning', Q_q), ('SARSA', Q_s), ('Expected SARSA', Q_e)]:
    policy = get_greedy_policy(Q, gw)
    visualize_gridworld(gw, policy=policy, title=f"{nome} - Pol√≠tica")

# Compara√ß√£o
compare_algorithms({'Q-Learning': Q_q, 'SARSA': Q_s, 'Expected SARSA': Q_e}, gw)

print("üéâ Experimento conclu√≠do!")
```

---

## üéØ Exemplos R√°pidos

### Exemplo 1: Q-Learning B√°sico

```python
from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld

gw = create_classic_gridworld()
Q, _ = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)
visualize_gridworld(gw, policy=get_greedy_policy(Q, gw))
```

### Exemplo 2: Comparar 3 Algoritmos

```python
from algorithms import sarsa, q_learning, expected_sarsa
from visualization import plot_learning_curves

gw = create_classic_gridworld()

_, r1 = sarsa(gw, n_episodes=1000)
_, r2 = q_learning(gw, n_episodes=1000)
_, r3 = expected_sarsa(gw, n_episodes=1000)

plot_learning_curves({
    'SARSA': r1,
    'Q-Learning': r2,
    'Expected SARSA': r3
})
```

### Exemplo 3: Cliff World

```python
from environment import create_cliff_world
from algorithms import sarsa, q_learning
from visualization import visualize_gridworld, get_greedy_policy

gw_cliff = create_cliff_world()

Q_s, _ = sarsa(gw_cliff, n_episodes=500)
Q_q, _ = q_learning(gw_cliff, n_episodes=500)

visualize_gridworld(gw_cliff, policy=get_greedy_policy(Q_s, gw_cliff), 
                   title="SARSA - Conservador")
visualize_gridworld(gw_cliff, policy=get_greedy_policy(Q_q, gw_cliff),
                   title="Q-Learning - Agressivo")
```

---

## üíæ Salvar no Google Drive (Persistente)

### Setup Inicial

```python
# Montar Drive
from google.colab import drive
drive.mount('/content/drive')

# Criar pasta
import os
pasta = '/content/drive/MyDrive/RL_GridWorld'
os.makedirs(pasta, exist_ok=True)

print(f"üìÅ Pasta: {pasta}")
print("üì§ Fa√ßa upload dos .py nesta pasta via Google Drive")
```

### Usar Arquivos do Drive

```python
# Adicionar ao path
import sys
sys.path.append('/content/drive/MyDrive/RL_GridWorld')

# Importar normalmente
from environment import create_classic_gridworld
from algorithms import q_learning

print("‚úÖ Importado do Drive!")
```

### Salvar Resultados

```python
import numpy as np

# Salvar Q-table
np.save('/content/drive/MyDrive/RL_GridWorld/Q_qlearning.npy', Q)

# Salvar rewards
np.save('/content/drive/MyDrive/RL_GridWorld/rewards.npy', rewards)

print("üíæ Resultados salvos no Drive!")
```

---

## üêõ Troubleshooting R√°pido

### ‚ùå Erro: "No module named 'environment'"

**Solu√ß√£o:**
```python
# Verificar arquivos
import os
print(os.listdir('.'))

# Se n√£o aparecer, fazer upload novamente
from google.colab import files
uploaded = files.upload()
```

### ‚ùå Erro: "name 'plt' is not defined"

**Solu√ß√£o:**
```python
import matplotlib.pyplot as plt
%matplotlib inline
```

### ‚ùå Gr√°ficos n√£o aparecem

**Solu√ß√£o:**
```python
# Sempre no in√≠cio do notebook
%matplotlib inline
import matplotlib.pyplot as plt
```

### ‚ùå MC Exploring Starts demora muito

**Solu√ß√£o:**
```python
# Adicionar max_steps
from algorithms import mc_exploring_starts
Q, rewards = mc_exploring_starts(gw, n_episodes=500, max_steps=200)
```

---

## üéì C√©lulas √öteis

### C√©lula: Reiniciar Ambiente

```python
# Limpar vari√°veis e reiniciar
%reset -f

# Reimportar
from environment import create_classic_gridworld
from algorithms import q_learning
from visualization import visualize_gridworld

print("üîÑ Ambiente reiniciado!")
```

### C√©lula: Ver Documenta√ß√£o

```python
# Ver ajuda de qualquer fun√ß√£o
help(q_learning)
help(create_classic_gridworld)
```

### C√©lula: Verificar Vers√µes

```python
import numpy as np
import matplotlib
import sys

print(f"Python: {sys.version}")
print(f"NumPy: {np.__version__}")
print(f"Matplotlib: {matplotlib.__version__}")
```

### C√©lula: Instalar Depend√™ncias (se necess√°rio)

```python
!pip install --upgrade numpy matplotlib
```

---

## üìä Experimentos Prontos

### Experimento 1: Sensibilidade ao Alpha

```python
alphas = [0.01, 0.05, 0.1, 0.3, 0.5]
results = {}

for alpha in alphas:
    _, rewards = q_learning(gw, n_episodes=500, alpha=alpha)
    results[f'Œ±={alpha}'] = rewards

plot_learning_curves(results, title="Sensibilidade ao Alpha")
```

### Experimento 2: Sensibilidade ao Epsilon

```python
epsilons = [0.0, 0.05, 0.1, 0.2, 0.3]
results = {}

for eps in epsilons:
    _, rewards = q_learning(gw, n_episodes=500, epsilon=eps)
    results[f'Œµ={eps}'] = rewards

plot_learning_curves(results, title="Sensibilidade ao Epsilon")
```

### Experimento 3: TD vs MC

```python
from algorithms import td_zero_prediction, first_visit_mc_prediction

policy = {s: 'L' for s in gw.states if not gw.is_terminal(s)}

V_td = td_zero_prediction(gw, policy, n_episodes=1000)
V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000)

visualize_gridworld(gw, values=V_td, title="TD(0)")
visualize_gridworld(gw, values=V_mc, title="Monte Carlo")
```

---

## ‚ö° Atalhos de Teclado no Colab

| A√ß√£o | Atalho |
|------|--------|
| Executar c√©lula | `Ctrl + Enter` |
| Executar e pr√≥xima | `Shift + Enter` |
| Nova c√©lula acima | `Ctrl + M A` |
| Nova c√©lula abaixo | `Ctrl + M B` |
| Deletar c√©lula | `Ctrl + M D` |
| Modo comando | `Esc` |
| Modo edi√ß√£o | `Enter` |

---

## üéØ Checklist

Antes de come√ßar seus experimentos:

- [ ] ‚úÖ Fez upload dos 3 arquivos .py
- [ ] ‚úÖ Executou os imports
- [ ] ‚úÖ Testou criar um ambiente
- [ ] ‚úÖ Treinou pelo menos um algoritmo
- [ ] ‚úÖ Visualizou os resultados

**Tudo OK? Voc√™ est√° pronto! üöÄ**

---

## üìö Links √öteis

- **üìñ Tutorial Completo:** [TUTORIAL.md](TUTORIAL.md)
- **üöÄ Guia R√°pido:** [GUIA_RAPIDO.md](GUIA_RAPIDO.md)
- **üèîÔ∏è Cliff World:** [GUIA_CLIFF_WORLD.md](GUIA_CLIFF_WORLD.md)
- **üìã Exemplos:** [EXEMPLOS_NOTEBOOK.md](EXEMPLOS_NOTEBOOK.md)

---

## üí° Dicas Finais

1. **Salve seu notebook** regularmente (Ctrl+S ou File > Save)
2. **Use coment√°rios** para documentar seus experimentos
3. **Copie o notebook** antes de fazer mudan√ßas grandes
4. **Organize em se√ß√µes** usando markdown
5. **Exporte resultados** para o Drive para n√£o perder

---

**Desenvolvido para facilitar seu aprendizado de RL! üéì**

**D√∫vidas? Consulte a [documenta√ß√£o completa](README_GITHUB.md)! üìö**
