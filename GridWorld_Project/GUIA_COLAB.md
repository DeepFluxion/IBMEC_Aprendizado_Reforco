# ğŸš€ Guia RÃ¡pido: Google Colab

## âš¡ Setup em 3 Passos (2 minutos)

### Passo 1ï¸âƒ£: Upload dos Arquivos

Cole esta cÃ©lula no Colab e execute:

```python
from google.colab import files
uploaded = files.upload()
print(f"âœ… {len(uploaded)} arquivo(s) carregado(s)")
```

**ğŸ‘† Clique em "Escolher arquivos" e selecione:**
- `environment.py`
- `algorithms.py`
- `visualization.py`

---

### Passo 2ï¸âƒ£: Imports

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld, plot_learning_curves

print("âœ… Pronto!")
```

---

### Passo 3ï¸âƒ£: Executar

```python
gw = create_classic_gridworld()
Q, rewards = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)
visualize_gridworld(gw, policy=get_greedy_policy(Q, gw))
plot_learning_curves({'Q-Learning': rewards})
```

**ğŸ‰ Pronto! VocÃª jÃ¡ estÃ¡ usando RL!**

---

## ğŸ“‹ Template Completo para Colab

Copie e cole este notebook completo:

```python
# =============================================================================
# REINFORCEMENT LEARNING - GRIDWORLD
# Template Completo para Google Colab
# =============================================================================

# -----------------------------------------------------------------------------
# 1ï¸âƒ£ UPLOAD DOS ARQUIVOS
# -----------------------------------------------------------------------------
from google.colab import files
print("ğŸ“¤ FaÃ§a upload dos 3 arquivos .py")
uploaded = files.upload()

# -----------------------------------------------------------------------------
# 2ï¸âƒ£ VERIFICAR ARQUIVOS
# -----------------------------------------------------------------------------
import os
arquivos = ['environment.py', 'algorithms.py', 'visualization.py']
ok = all(f in os.listdir('.') for f in arquivos)
print("âœ… Arquivos OK" if ok else "âŒ Faltam arquivos")

# -----------------------------------------------------------------------------
# 3ï¸âƒ£ IMPORTS
# -----------------------------------------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

from environment import create_classic_gridworld, create_cliff_world
from algorithms import sarsa, q_learning, expected_sarsa, get_greedy_policy
from visualization import (
    visualize_gridworld, 
    plot_learning_curves, 
    compare_algorithms
)

print("âœ… MÃ³dulos importados!")

# -----------------------------------------------------------------------------
# 4ï¸âƒ£ CRIAR AMBIENTE
# -----------------------------------------------------------------------------
gw = create_classic_gridworld()
visualize_gridworld(gw, title="GridWorld 4x3")

# -----------------------------------------------------------------------------
# 5ï¸âƒ£ TREINAR ALGORITMOS
# -----------------------------------------------------------------------------
print("ğŸ§  Treinando...")

Q_q, rewards_q = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)
Q_s, rewards_s = sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)
Q_e, rewards_e = expected_sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)

print("âœ… Treinamento concluÃ­do!")

# -----------------------------------------------------------------------------
# 6ï¸âƒ£ VISUALIZAR RESULTADOS
# -----------------------------------------------------------------------------
print("ğŸ“Š Visualizando...")

# Curvas de aprendizado
plot_learning_curves({
    'Q-Learning': rewards_q,
    'SARSA': rewards_s,
    'Expected SARSA': rewards_e
})

# PolÃ­ticas
for nome, Q in [('Q-Learning', Q_q), ('SARSA', Q_s), ('Expected SARSA', Q_e)]:
    policy = get_greedy_policy(Q, gw)
    visualize_gridworld(gw, policy=policy, title=f"{nome} - PolÃ­tica")

# ComparaÃ§Ã£o
compare_algorithms({'Q-Learning': Q_q, 'SARSA': Q_s, 'Expected SARSA': Q_e}, gw)

print("ğŸ‰ Experimento concluÃ­do!")
```

---

## ğŸ¯ Exemplos RÃ¡pidos

### Exemplo 1: Q-Learning BÃ¡sico

```python
from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld

gw = create_classic_gridworld()
Q, _ = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)
visualize_gridworld(gw, policy=get_greedy_policy(Q, gw))
```

### Exemplo 2: Comparar 3 Algoritmos

```python
from algorithms import sarsa, q_learning, expected_sarsa
from visualization import plot_learning_curves

gw = create_classic_gridworld()

_, r1 = sarsa(gw, n_episodes=1000)
_, r2 = q_learning(gw, n_episodes=1000)
_, r3 = expected_sarsa(gw, n_episodes=1000)

plot_learning_curves({
    'SARSA': r1,
    'Q-Learning': r2,
    'Expected SARSA': r3
})
```

### Exemplo 3: Cliff World

```python
from environment import create_cliff_world
from algorithms import sarsa, q_learning
from visualization import visualize_gridworld, get_greedy_policy

gw_cliff = create_cliff_world()

Q_s, _ = sarsa(gw_cliff, n_episodes=500)
Q_q, _ = q_learning(gw_cliff, n_episodes=500)

visualize_gridworld(gw_cliff, policy=get_greedy_policy(Q_s, gw_cliff), 
                   title="SARSA - Conservador")
visualize_gridworld(gw_cliff, policy=get_greedy_policy(Q_q, gw_cliff),
                   title="Q-Learning - Agressivo")
```

---

## ğŸ’¾ Salvar no Google Drive (Persistente)

### Setup Inicial

```python
# Montar Drive
from google.colab import drive
drive.mount('/content/drive')

# Criar pasta
import os
pasta = '/content/drive/MyDrive/RL_GridWorld'
os.makedirs(pasta, exist_ok=True)

print(f"ğŸ“ Pasta: {pasta}")
print("ğŸ“¤ FaÃ§a upload dos .py nesta pasta via Google Drive")
```

### Usar Arquivos do Drive

```python
# Adicionar ao path
import sys
sys.path.append('/content/drive/MyDrive/RL_GridWorld')

# Importar normalmente
from environment import create_classic_gridworld
from algorithms import q_learning

print("âœ… Importado do Drive!")
```

### Salvar Resultados

```python
import numpy as np

# Salvar Q-table
np.save('/content/drive/MyDrive/RL_GridWorld/Q_qlearning.npy', Q)

# Salvar rewards
np.save('/content/drive/MyDrive/RL_GridWorld/rewards.npy', rewards)

print("ğŸ’¾ Resultados salvos no Drive!")
```

---

## ğŸ› Troubleshooting RÃ¡pido

### âŒ Erro: "No module named 'environment'"

**SoluÃ§Ã£o:**
```python
# Verificar arquivos
import os
print(os.listdir('.'))

# Se nÃ£o aparecer, fazer upload novamente
from google.colab import files
uploaded = files.upload()
```

### âŒ Erro: "name 'plt' is not defined"

**SoluÃ§Ã£o:**
```python
import matplotlib.pyplot as plt
%matplotlib inline
```

### âŒ GrÃ¡ficos nÃ£o aparecem

**SoluÃ§Ã£o:**
```python
# Sempre no inÃ­cio do notebook
%matplotlib inline
import matplotlib.pyplot as plt
```

### âŒ MC Exploring Starts demora muito

**SoluÃ§Ã£o:**
```python
# Adicionar max_steps
from algorithms import mc_exploring_starts
Q, rewards = mc_exploring_starts(gw, n_episodes=500, max_steps=200)
```

---

## ğŸ“ CÃ©lulas Ãšteis

### CÃ©lula: Reiniciar Ambiente

```python
# Limpar variÃ¡veis e reiniciar
%reset -f

# Reimportar
from environment import create_classic_gridworld
from algorithms import q_learning
from visualization import visualize_gridworld

print("ğŸ”„ Ambiente reiniciado!")
```

### CÃ©lula: Ver DocumentaÃ§Ã£o

```python
# Ver ajuda de qualquer funÃ§Ã£o
help(q_learning)
help(create_classic_gridworld)
```

### CÃ©lula: Verificar VersÃµes

```python
import numpy as np
import matplotlib
import sys

print(f"Python: {sys.version}")
print(f"NumPy: {np.__version__}")
print(f"Matplotlib: {matplotlib.__version__}")
```

### CÃ©lula: Instalar DependÃªncias (se necessÃ¡rio)

```python
!pip install --upgrade numpy matplotlib
```

---

## ğŸ“Š Experimentos Prontos

### Experimento 1: Sensibilidade ao Alpha

```python
alphas = [0.01, 0.05, 0.1, 0.3, 0.5]
results = {}

for alpha in alphas:
    _, rewards = q_learning(gw, n_episodes=500, alpha=alpha)
    results[f'Î±={alpha}'] = rewards

plot_learning_curves(results, title="Sensibilidade ao Alpha")
```

### Experimento 2: Sensibilidade ao Epsilon

```python
epsilons = [0.0, 0.05, 0.1, 0.2, 0.3]
results = {}

for eps in epsilons:
    _, rewards = q_learning(gw, n_episodes=500, epsilon=eps)
    results[f'Îµ={eps}'] = rewards

plot_learning_curves(results, title="Sensibilidade ao Epsilon")
```

### Experimento 3: TD vs MC

```python
from algorithms import td_zero_prediction, first_visit_mc_prediction

policy = {s: 'L' for s in gw.states if not gw.is_terminal(s)}

V_td = td_zero_prediction(gw, policy, n_episodes=1000)
V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000)

visualize_gridworld(gw, values=V_td, title="TD(0)")
visualize_gridworld(gw, values=V_mc, title="Monte Carlo")
```

---

## âš¡ Atalhos de Teclado no Colab

| AÃ§Ã£o | Atalho |
|------|--------|
| Executar cÃ©lula | `Ctrl + Enter` |
| Executar e prÃ³xima | `Shift + Enter` |
| Nova cÃ©lula acima | `Ctrl + M A` |
| Nova cÃ©lula abaixo | `Ctrl + M B` |
| Deletar cÃ©lula | `Ctrl + M D` |
| Modo comando | `Esc` |
| Modo ediÃ§Ã£o | `Enter` |

---

## ğŸ¯ Checklist

Antes de comeÃ§ar seus experimentos:

- [ ] âœ… Fez upload dos 3 arquivos .py
- [ ] âœ… Executou os imports
- [ ] âœ… Testou criar um ambiente
- [ ] âœ… Treinou pelo menos um algoritmo
- [ ] âœ… Visualizou os resultados

**Tudo OK? VocÃª estÃ¡ pronto! ğŸš€**

---

## ğŸ“š Links Ãšteis

- **ğŸ“– Tutorial Completo:** [TUTORIAL.md](TUTORIAL.md)
- **ğŸš€ Guia RÃ¡pido:** [GUIA_RAPIDO.md](GUIA_RAPIDO.md)
- **ğŸ”ï¸ Cliff World:** [GUIA_CLIFF_WORLD.md](GUIA_CLIFF_WORLD.md)
- **ğŸ“‹ Exemplos:** [EXEMPLOS_NOTEBOOK.md](EXEMPLOS_NOTEBOOK.md)

---

## ğŸ’¡ Dicas Finais

1. **Salve seu notebook** regularmente (Ctrl+S ou File > Save)
2. **Use comentÃ¡rios** para documentar seus experimentos
3. **Copie o notebook** antes de fazer mudanÃ§as grandes
4. **Organize em seÃ§Ãµes** usando markdown
5. **Exporte resultados** para o Drive para nÃ£o perder

---

**Desenvolvido para facilitar seu aprendizado de RL! ğŸ“**

**DÃºvidas? Consulte a [documentaÃ§Ã£o completa](README_GITHUB.md)! ğŸ“š**
