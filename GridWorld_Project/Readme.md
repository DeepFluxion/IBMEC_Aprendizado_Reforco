# üéì Reinforcement Learning GridWorld - Tutorial Completo

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seu-usuario/seu-repo)

Pacote completo para aprender **Reinforcement Learning** usando ambientes GridWorld. Inclui 7 algoritmos implementados, 12 fun√ß√µes de visualiza√ß√£o e documenta√ß√£o extensiva.

---

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Funcionalidades](#funcionalidades)
- [Instala√ß√£o Local](#instala√ß√£o-local)
- [üöÄ Usando no Google Colab](#usando-no-google-colab)
- [Quick Start](#quick-start)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Documenta√ß√£o](#documenta√ß√£o)
- [Exemplos](#exemplos)
- [Contribuindo](#contribuindo)
- [Licen√ßa](#licen√ßa)

---

## üéØ Vis√£o Geral

Este projeto fornece uma implementa√ß√£o educacional completa de algoritmos de Reinforcement Learning, incluindo:

- **3 m√≥dulos Python** prontos para usar
- **7 algoritmos** implementados (TD, MC, SARSA, Q-Learning, etc.)
- **12 fun√ß√µes de visualiza√ß√£o** profissionais
- **Documenta√ß√£o extensiva** com mais de 50 exemplos
- **Notebooks interativos** no Google Colab

### Por que este projeto?

‚úÖ **Focado em educa√ß√£o** - C√≥digo limpo e did√°tico  
‚úÖ **Pronto para usar** - Zero configura√ß√£o  
‚úÖ **Totalmente documentado** - help() em todas as fun√ß√µes  
‚úÖ **Google Colab friendly** - Execute na nuvem gratuitamente  

---

## ‚ö° Funcionalidades

### Ambientes
- GridWorld 4x3 cl√°ssico (Russell & Norvig)
- Cliff World configur√°vel
- Cria√ß√£o de ambientes personalizados

### Algoritmos de Predi√ß√£o
- TD(0) - Temporal Difference
- First-Visit Monte Carlo Prediction

### Algoritmos de Controle
- SARSA (on-policy)
- Q-Learning (off-policy)
- Expected SARSA
- First-Visit Monte Carlo Control
- Monte Carlo Exploring Starts

### Visualiza√ß√µes
- Grids com valores e pol√≠ticas
- Q-values detalhados
- Curvas de aprendizado
- Heatmaps
- Compara√ß√µes entre algoritmos

---

## üíª Instala√ß√£o Local

### Requisitos
- Python 3.7+
- NumPy
- Matplotlib

### Passo a Passo

1. **Clone o reposit√≥rio:**
```bash
git clone https://github.com/seu-usuario/rl-gridworld.git
cd rl-gridworld
```

2. **Instale as depend√™ncias:**
```bash
pip install numpy matplotlib
```

3. **Use em seu notebook Jupyter:**
```python
from environment import create_classic_gridworld
from algorithms import q_learning
from visualization import visualize_gridworld

# Pronto para usar!
gw = create_classic_gridworld()
```

---

## üöÄ Usando no Google Colab

### Op√ß√£o 1: Upload Manual dos Arquivos (Mais Simples)

#### Passo 1: Fazer Upload dos M√≥dulos

No Google Colab, execute esta c√©lula para fazer upload dos arquivos:

```python
# C√©lula 1: Upload dos arquivos Python
from google.colab import files

print("üì§ Fa√ßa upload dos 3 arquivos Python:")
print("   1. environment.py")
print("   2. algorithms.py")
print("   3. visualization.py")
print()

uploaded = files.upload()

print("\n‚úÖ Arquivos carregados com sucesso!")
print(f"   Total: {len(uploaded)} arquivo(s)")
```

**Instru√ß√µes:**
1. Execute a c√©lula acima
2. Clique em "Escolher arquivos"
3. Selecione os 3 arquivos `.py` do seu computador:
   - `environment.py`
   - `algorithms.py`
   - `visualization.py`
4. Aguarde o upload (alguns segundos)

#### Passo 2: Verificar Upload

```python
# C√©lula 2: Verificar se arquivos foram carregados
import os

arquivos_necessarios = ['environment.py', 'algorithms.py', 'visualization.py']
arquivos_presentes = [f for f in arquivos_necessarios if os.path.exists(f)]

print("üìã Verifica√ß√£o de Arquivos:")
print("-" * 50)

for arquivo in arquivos_necessarios:
    if arquivo in arquivos_presentes:
        print(f"‚úÖ {arquivo} - OK")
    else:
        print(f"‚ùå {arquivo} - FALTANDO")

if len(arquivos_presentes) == 3:
    print("\nüéâ Todos os arquivos est√£o presentes!")
    print("   Voc√™ pode prosseguir para os imports.")
else:
    print("\n‚ö†Ô∏è  Arquivos faltando! Fa√ßa upload novamente.")
```

#### Passo 3: Importar M√≥dulos

```python
# C√©lula 3: Importar os m√≥dulos
import numpy as np
import matplotlib.pyplot as plt

# Configura√ß√£o do matplotlib
%matplotlib inline
plt.rcParams['figure.figsize'] = (10, 6)

# Importar m√≥dulos do projeto
from environment import (
    GridWorld,
    create_classic_gridworld,
    create_custom_gridworld,
    create_cliff_world,
    print_gridworld_info
)

from algorithms import (
    # Predi√ß√£o
    td_zero_prediction,
    first_visit_mc_prediction,
    # Controle
    sarsa,
    q_learning,
    expected_sarsa,
    first_visit_mc_control,
    mc_exploring_starts,
    # Auxiliares
    get_greedy_policy
)

from visualization import (
    visualize_gridworld,
    visualize_q_values,
    visualize_q_table_detailed,
    plot_learning_curves,
    plot_value_heatmap,
    plot_q_value_heatmap,
    compare_algorithms,
    print_q_table
)

print("‚úÖ Todos os m√≥dulos importados com sucesso!")
print("üöÄ Pronto para come√ßar!")
```

#### Passo 4: Executar Quick Start

```python
# C√©lula 4: Quick Start - Primeiro Experimento
print("üéØ Quick Start - Treinando Q-Learning")
print("=" * 60)

# Criar ambiente
gw = create_classic_gridworld()
print("\n‚úì Ambiente criado")

# Treinar Q-Learning
Q, rewards = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1, verbose=True)
print("\n‚úì Treinamento conclu√≠do")

# Extrair pol√≠tica
policy = get_greedy_policy(Q, gw)
print("‚úì Pol√≠tica extra√≠da")

# Visualizar
print("\nüìä Visualizando resultados...")
visualize_gridworld(gw, policy=policy, title="Q-Learning - Pol√≠tica Aprendida")
plot_learning_curves({'Q-Learning': rewards}, title="Curva de Aprendizado")

print("\nüéâ Primeiro experimento conclu√≠do com sucesso!")
```

---

### Op√ß√£o 2: Clone do GitHub (Autom√°tico)

Se os arquivos estiverem em um reposit√≥rio GitHub p√∫blico:

```python
# C√©lula 1: Clonar reposit√≥rio
!git clone https://github.com/seu-usuario/rl-gridworld.git
%cd rl-gridworld

# Verificar arquivos
!ls -la *.py
```

```python
# C√©lula 2: Importar (igual √† Op√ß√£o 1, C√©lula 3)
import numpy as np
import matplotlib.pyplot as plt

from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld, plot_learning_curves

print("‚úÖ M√≥dulos importados!")
```

---

### Op√ß√£o 3: Google Drive (Persistente)

Para manter os arquivos entre sess√µes:

#### Passo 1: Montar Google Drive

```python
# C√©lula 1: Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

print("‚úÖ Google Drive montado!")
```

#### Passo 2: Criar pasta e fazer upload

```python
# C√©lula 2: Criar estrutura de pastas
import os

# Criar pasta no Drive (se n√£o existir)
pasta_projeto = '/content/drive/MyDrive/RL_GridWorld'
os.makedirs(pasta_projeto, exist_ok=True)

print(f"üìÅ Pasta criada/verificada: {pasta_projeto}")
print("\nüì§ Fa√ßa upload dos 3 arquivos .py para esta pasta:")
print("   1. Abra Google Drive")
print("   2. Navegue at√©: Meu Drive > RL_GridWorld")
print("   3. Fa√ßa upload de environment.py, algorithms.py, visualization.py")
```

#### Passo 3: Adicionar ao path e importar

```python
# C√©lula 3: Adicionar pasta ao path do Python
import sys
sys.path.append('/content/drive/MyDrive/RL_GridWorld')

# Importar m√≥dulos
from environment import create_classic_gridworld
from algorithms import q_learning
from visualization import visualize_gridworld

print("‚úÖ M√≥dulos importados do Google Drive!")
print("üíæ Os arquivos permanecer√£o salvos para pr√≥ximas sess√µes")
```

---

## üéì Tutorial Passo a Passo no Colab

### Notebook Completo de Exemplo

Aqui est√° um notebook completo que voc√™ pode copiar para o Colab:

```python
# =============================================================================
# NOTEBOOK: Reinforcement Learning GridWorld - Tutorial Completo
# =============================================================================

# -----------------------------------------------------------------------------
# C√âLULA 1: Upload e Setup
# -----------------------------------------------------------------------------
from google.colab import files
import os

print("üì§ PASSO 1: Upload dos Arquivos")
print("=" * 60)
print("Fa√ßa upload dos 3 arquivos Python:")
print("  ‚Ä¢ environment.py")
print("  ‚Ä¢ algorithms.py")
print("  ‚Ä¢ visualization.py")
print()

uploaded = files.upload()

# Verificar
arquivos_ok = all(f in uploaded for f in ['environment.py', 'algorithms.py', 'visualization.py'])
if arquivos_ok:
    print("\n‚úÖ Todos os arquivos carregados!")
else:
    print("\n‚ö†Ô∏è  Alguns arquivos est√£o faltando. Fa√ßa upload novamente.")

# -----------------------------------------------------------------------------
# C√âLULA 2: Imports e Configura√ß√£o
# -----------------------------------------------------------------------------
print("üì¶ PASSO 2: Importando M√≥dulos")
print("=" * 60)

import numpy as np
import matplotlib.pyplot as plt

# Configura√ß√£o do matplotlib para Colab
%matplotlib inline
plt.rcParams['figure.figsize'] = (12, 6)

# Imports do projeto
from environment import (
    create_classic_gridworld,
    create_cliff_world,
    print_gridworld_info
)

from algorithms import (
    sarsa,
    q_learning,
    expected_sarsa,
    get_greedy_policy
)

from visualization import (
    visualize_gridworld,
    visualize_q_values,
    plot_learning_curves,
    compare_algorithms
)

print("‚úÖ Imports conclu√≠dos!")

# -----------------------------------------------------------------------------
# C√âLULA 3: Criar e Visualizar Ambiente
# -----------------------------------------------------------------------------
print("\nüèóÔ∏è  PASSO 3: Criando Ambiente")
print("=" * 60)

gw = create_classic_gridworld()
print_gridworld_info(gw)

visualize_gridworld(gw, title="GridWorld 4x3 Cl√°ssico")

# -----------------------------------------------------------------------------
# C√âLULA 4: Treinar Q-Learning
# -----------------------------------------------------------------------------
print("\nüß† PASSO 4: Treinando Q-Learning")
print("=" * 60)

Q_qlearning, rewards_qlearning = q_learning(
    gridworld=gw,
    n_episodes=1000,
    alpha=0.1,
    gamma=0.9,
    epsilon=0.1,
    verbose=True
)

print("\n‚úÖ Treinamento Q-Learning conclu√≠do!")

# -----------------------------------------------------------------------------
# C√âLULA 5: Treinar SARSA
# -----------------------------------------------------------------------------
print("\nüß† PASSO 5: Treinando SARSA")
print("=" * 60)

Q_sarsa, rewards_sarsa = sarsa(
    gridworld=gw,
    n_episodes=1000,
    alpha=0.1,
    gamma=0.9,
    epsilon=0.1,
    verbose=True
)

print("\n‚úÖ Treinamento SARSA conclu√≠do!")

# -----------------------------------------------------------------------------
# C√âLULA 6: Treinar Expected SARSA
# -----------------------------------------------------------------------------
print("\nüß† PASSO 6: Treinando Expected SARSA")
print("=" * 60)

Q_expected, rewards_expected = expected_sarsa(
    gridworld=gw,
    n_episodes=1000,
    alpha=0.1,
    gamma=0.9,
    epsilon=0.1,
    verbose=True
)

print("\n‚úÖ Treinamento Expected SARSA conclu√≠do!")

# -----------------------------------------------------------------------------
# C√âLULA 7: Visualizar Pol√≠ticas
# -----------------------------------------------------------------------------
print("\nüìä PASSO 7: Visualizando Pol√≠ticas")
print("=" * 60)

# Extrair pol√≠ticas
policy_qlearning = get_greedy_policy(Q_qlearning, gw)
policy_sarsa = get_greedy_policy(Q_sarsa, gw)
policy_expected = get_greedy_policy(Q_expected, gw)

# Visualizar
visualize_gridworld(gw, policy=policy_qlearning, title="Q-Learning - Pol√≠tica")
visualize_gridworld(gw, policy=policy_sarsa, title="SARSA - Pol√≠tica")
visualize_gridworld(gw, policy=policy_expected, title="Expected SARSA - Pol√≠tica")

# -----------------------------------------------------------------------------
# C√âLULA 8: Comparar Algoritmos
# -----------------------------------------------------------------------------
print("\nüìà PASSO 8: Comparando Algoritmos")
print("=" * 60)

# Curvas de aprendizado
plot_learning_curves({
    'Q-Learning': rewards_qlearning,
    'SARSA': rewards_sarsa,
    'Expected SARSA': rewards_expected
}, window=100, title="Compara√ß√£o de Converg√™ncia")

# Compara√ß√£o num√©rica
compare_algorithms({
    'Q-Learning': Q_qlearning,
    'SARSA': Q_sarsa,
    'Expected SARSA': Q_expected
}, gw)

# -----------------------------------------------------------------------------
# C√âLULA 9: Visualizar Q-Values
# -----------------------------------------------------------------------------
print("\nüé® PASSO 9: Visualizando Q-Values")
print("=" * 60)

visualize_q_values(Q_qlearning, gw, title="Q-Learning - Max Q-Values")
visualize_q_values(Q_sarsa, gw, title="SARSA - Max Q-Values")
visualize_q_values(Q_expected, gw, title="Expected SARSA - Max Q-Values")

# -----------------------------------------------------------------------------
# C√âLULA 10: Conclus√£o
# -----------------------------------------------------------------------------
print("\n" + "=" * 60)
print("üéâ TUTORIAL CONCLU√çDO COM SUCESSO!")
print("=" * 60)
print("\n‚úÖ Voc√™ aprendeu a:")
print("   ‚Ä¢ Fazer upload de m√≥dulos no Colab")
print("   ‚Ä¢ Criar ambientes GridWorld")
print("   ‚Ä¢ Treinar algoritmos de RL")
print("   ‚Ä¢ Visualizar resultados")
print("   ‚Ä¢ Comparar algoritmos")
print("\nüìö Pr√≥ximos passos:")
print("   ‚Ä¢ Experimente par√¢metros diferentes")
print("   ‚Ä¢ Crie ambientes personalizados")
print("   ‚Ä¢ Teste o Cliff World")
print("   ‚Ä¢ Fa√ßa seus pr√≥prios experimentos!")
print("\nüöÄ Boa sorte com seus estudos de RL!")
```

---

## üìÅ Estrutura do Projeto

```
rl-gridworld/
‚îÇ
‚îú‚îÄ‚îÄ üì¶ M√≥dulos Python
‚îÇ   ‚îú‚îÄ‚îÄ environment.py          # Ambientes GridWorld
‚îÇ   ‚îú‚îÄ‚îÄ algorithms.py           # 7 algoritmos de RL
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py        # 12 fun√ß√µes de visualiza√ß√£o
‚îÇ
‚îú‚îÄ‚îÄ üìì Notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_quick_start.ipynb    # Introdu√ß√£o r√°pida
‚îÇ   ‚îú‚îÄ‚îÄ 02_predition.ipynb      # Algoritmos de predi√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ 03_control.ipynb        # Algoritmos de controle
‚îÇ   ‚îú‚îÄ‚îÄ 04_comparison.ipynb     # Compara√ß√£o de algoritmos
‚îÇ   ‚îî‚îÄ‚îÄ 05_cliff_world.ipynb    # Experimentos com Cliff World
‚îÇ
‚îú‚îÄ‚îÄ üìö Documenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ INDEX.md                # Mapa do projeto
‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Este arquivo
‚îÇ   ‚îú‚îÄ‚îÄ TUTORIAL.md             # Tutorial completo
‚îÇ   ‚îú‚îÄ‚îÄ GUIA_RAPIDO.md          # Refer√™ncia r√°pida
‚îÇ   ‚îú‚îÄ‚îÄ EXEMPLOS_NOTEBOOK.md    # C√©lulas prontas
‚îÇ   ‚îî‚îÄ‚îÄ GUIA_CLIFF_WORLD.md     # Guia do Cliff World
‚îÇ
‚îî‚îÄ‚îÄ üìÑ Outros
    ‚îú‚îÄ‚îÄ LICENSE                 # Licen√ßa MIT
    ‚îî‚îÄ‚îÄ requirements.txt        # Depend√™ncias
```

---

## üìñ Documenta√ß√£o

### Documenta√ß√£o Online
- **[INDEX.md](INDEX.md)** - Mapa completo do projeto (COMECE AQUI)
- **[TUTORIAL.md](TUTORIAL.md)** - Tutorial completo com 50+ exemplos
- **[GUIA_RAPIDO.md](GUIA_RAPIDO.md)** - Refer√™ncia r√°pida
- **[EXEMPLOS_NOTEBOOK.md](EXEMPLOS_NOTEBOOK.md)** - 20 c√©lulas prontas

### Documenta√ß√£o Integrada
Todas as fun√ß√µes t√™m documenta√ß√£o completa:

```python
# Ver documenta√ß√£o
help(q_learning)
help(create_classic_gridworld)
help(visualize_gridworld)
```

---

## üéØ Quick Start

### Exemplo M√≠nimo (3 linhas)

```python
from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld

gw = create_classic_gridworld()
Q, _ = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)
visualize_gridworld(gw, policy=get_greedy_policy(Q, gw))
```

### Exemplo Completo

```python
import numpy as np
import matplotlib.pyplot as plt

from environment import create_classic_gridworld, print_gridworld_info
from algorithms import q_learning, sarsa, get_greedy_policy
from visualization import visualize_gridworld, plot_learning_curves, compare_algorithms

# 1. Criar ambiente
gw = create_classic_gridworld()
print_gridworld_info(gw)

# 2. Treinar algoritmos
Q_q, rewards_q = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)
Q_s, rewards_s = sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# 3. Visualizar resultados
plot_learning_curves({'Q-Learning': rewards_q, 'SARSA': rewards_s})
compare_algorithms({'Q-Learning': Q_q, 'SARSA': Q_s}, gw)

# 4. Visualizar pol√≠ticas
policy_q = get_greedy_policy(Q_q, gw)
policy_s = get_greedy_policy(Q_s, gw)
visualize_gridworld(gw, policy=policy_q, title="Q-Learning")
visualize_gridworld(gw, policy=policy_s, title="SARSA")
```

---

## üí° Exemplos

### Criar Ambientes Diferentes

```python
# GridWorld 4x3 cl√°ssico
gw_classic = create_classic_gridworld()

# Grid personalizado
gw_custom = create_custom_gridworld(
    rows=5, cols=5,
    walls=[(2, 2), (2, 3)],
    terminals={(0, 4): 10.0},
    gamma=0.95
)

# Cliff World
gw_cliff = create_cliff_world()

# Cliff World customizado
gw_cliff_custom = create_cliff_world(
    rows=6, cols=12,
    cliff_reward=-50.0,
    noise=0.1
)
```

### Comparar Todos os Algoritmos

```python
from algorithms import (
    sarsa, q_learning, expected_sarsa,
    first_visit_mc_control, mc_exploring_starts
)

algoritmos = {
    'SARSA': lambda: sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1),
    'Q-Learning': lambda: q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1),
    'Expected SARSA': lambda: expected_sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1),
    'MC Control': lambda: first_visit_mc_control(gw, n_episodes=1000, alpha=0.1, epsilon=0.1),
    'MC ES': lambda: mc_exploring_starts(gw, n_episodes=1000, alpha=0.1, max_steps=500)
}

resultados = {}
for nome, algoritmo in algoritmos.items():
    print(f"Treinando {nome}...")
    Q, rewards = algoritmo()
    resultados[nome] = rewards

plot_learning_curves(resultados, title="Compara√ß√£o de Todos os Algoritmos")
```

---

## üêõ Troubleshooting

### Problema: M√≥dulos n√£o encontrados no Colab

**Solu√ß√£o:**
```python
# Verificar se arquivos existem
import os
print(os.listdir('.'))  # Deve mostrar os 3 arquivos .py

# Se n√£o aparecerem, fa√ßa upload novamente
from google.colab import files
uploaded = files.upload()
```

### Problema: Erro de import

**Solu√ß√£o:**
```python
# Reinstalar depend√™ncias
!pip install --upgrade numpy matplotlib

# Reiniciar o runtime: Runtime > Restart Runtime
```

### Problema: Gr√°ficos n√£o aparecem

**Solu√ß√£o:**
```python
# Adicionar no in√≠cio do notebook
%matplotlib inline
import matplotlib.pyplot as plt
```

### Problema: MC Exploring Starts muito lento

**Solu√ß√£o:**
```python
# Usar par√¢metro max_steps
Q, rewards = mc_exploring_starts(gw, n_episodes=1000, max_steps=500)

# Ou diminuir n√∫mero de epis√≥dios
Q, rewards = mc_exploring_starts(gw, n_episodes=500, max_steps=200)
```

---

## üìù Notebooks Dispon√≠veis

### 1. Quick Start (01_quick_start.ipynb)
- Introdu√ß√£o ao projeto
- Primeiro experimento
- Q-Learning b√°sico

### 2. Predi√ß√£o (02_prediction.ipynb)
- TD(0)
- Monte Carlo
- Avalia√ß√£o de pol√≠ticas

### 3. Controle (03_control.ipynb)
- SARSA
- Q-Learning
- Expected SARSA
- Monte Carlo Control

### 4. Compara√ß√£o (04_comparison.ipynb)
- Comparar todos os algoritmos
- An√°lise de sensibilidade
- M√∫ltiplas runs

### 5. Cliff World (05_cliff_world.ipynb)
- Experimentos com Cliff World
- SARSA vs Q-Learning
- Configura√ß√µes diferentes

**Abrir no Colab:**
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seu-usuario/rl-gridworld)

---

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### √Åreas para Contribui√ß√£o
- Novos algoritmos (Double Q-Learning, etc.)
- Novos ambientes (Windy GridWorld, etc.)
- Melhorias na visualiza√ß√£o
- Mais exemplos e notebooks
- Tradu√ß√µes da documenta√ß√£o
- Corre√ß√µes de bugs

---

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

```
MIT License

Copyright (c) 2025 [Seu Nome]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## üìû Contato e Suporte

### Documenta√ß√£o
- üìñ [Tutorial Completo](TUTORIAL.md)
- üöÄ [Guia R√°pido](GUIA_RAPIDO.md)
- üìã [√çndice Completo](INDEX.md)

### Ajuda
- üêõ [Issues](https://github.com/seu-usuario/rl-gridworld/issues)
- üí¨ [Discussions](https://github.com/seu-usuario/rl-gridworld/discussions)
- üìß Email: seu-email@example.com

### Recursos
- üìö [Sutton & Barto - RL Book](http://incompleteideas.net/book/the-book.html)
- üéì [OpenAI Spinning Up](https://spinningup.openai.com/)
- üî¨ [DeepMind RL Course](https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021)

---

## üåü Reconhecimentos

Este projeto foi inspirado por:
- **Sutton & Barto** - Reinforcement Learning: An Introduction
- **Russell & Norvig** - Artificial Intelligence: A Modern Approach
- Comunidade de RL no GitHub

---

## üìä Estat√≠sticas do Projeto

- **Linhas de c√≥digo:** ~2.000
- **Fun√ß√µes:** 30+
- **Algoritmos:** 7
- **Documenta√ß√£o:** 70+ KB
- **Exemplos:** 50+
- **Notebooks:** 5

---

## üéì Para Educadores

Este projeto √© ideal para:
- Cursos de Intelig√™ncia Artificial
- Disciplinas de Aprendizado por Refor√ßo
- Workshops e tutoriais
- Trabalhos pr√°ticos de estudantes

**Recursos para professores:**
- Notebooks prontos para aula
- Exerc√≠cios pr√°ticos
- Visualiza√ß√µes did√°ticas
- Documenta√ß√£o extensiva

---

## üöÄ Roadmap

### Vers√£o Atual (v1.0)
- ‚úÖ 7 algoritmos implementados
- ‚úÖ 3 ambientes prontos
- ‚úÖ Documenta√ß√£o completa
- ‚úÖ Suporte ao Google Colab

### Pr√≥ximas Vers√µes

**v1.1**
- [ ] Double Q-Learning
- [ ] Windy GridWorld
- [ ] Testes unit√°rios
- [ ] CI/CD

**v1.2**
- [ ] N-Step methods
- [ ] Eligibility traces
- [ ] Function approximation
- [ ] PyPI package

**v2.0**
- [ ] Deep RL (DQN)
- [ ] Policy Gradient
- [ ] Actor-Critic
- [ ] Ambientes Gym

---

## ‚≠ê Star History

Se este projeto foi √∫til para voc√™, considere dar uma ‚≠ê!

[![Star History Chart](https://api.star-history.com/svg?repos=seu-usuario/rl-gridworld&type=Date)](https://star-history.com/#seu-usuario/rl-gridworld&Date)

---

## üìà Status do Projeto

![Build Status](https://img.shields.io/badge/build-passing-brightgreen)
![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen)
![Code Quality](https://img.shields.io/badge/code%20quality-A-brightgreen)
![Documentation](https://img.shields.io/badge/docs-100%25-brightgreen)

---

<div align="center">

**Desenvolvido com ‚ù§Ô∏è para a comunidade de Reinforcement Learning**

[üè† Home](https://github.com/seu-usuario/rl-gridworld) ‚Ä¢ 
[üìñ Docs](TUTORIAL.md) ‚Ä¢ 
[üöÄ Colab](https://colab.research.google.com/github/seu-usuario/rl-gridworld) ‚Ä¢ 
[üí¨ Discuss√µes](https://github.com/seu-usuario/rl-gridworld/discussions)

**Criado em 2025**

</div>
