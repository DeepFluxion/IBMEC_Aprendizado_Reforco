{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepFluxion/IBMEC_Aprendizado_Reforco/blob/main/GridWorld_Project/Aula_18_Tutorial_Completo_MC_TD_GW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b2d804d",
      "metadata": {
        "id": "7b2d804d"
      },
      "source": [
        "# Tutorial: Usando os M√≥dulos de Reinforcement Learning\n",
        "\n",
        "## üìö √çndice\n",
        "1. [Vis√£o Geral](#vis√£o-geral)\n",
        "2. [Estrutura dos M√≥dulos](#estrutura-dos-m√≥dulos)\n",
        "3. [Criando Ambientes GridWorld](#criando-ambientes-gridworld)\n",
        "4. [Criando Pol√≠ticas](#criando-pol√≠ticas)\n",
        "5. [Algoritmos de Predi√ß√£o](#algoritmos-de-predi√ß√£o)\n",
        "6. [Algoritmos de Controle](#algoritmos-de-controle)\n",
        "7. [Visualiza√ß√£o de Resultados](#visualiza√ß√£o-de-resultados)\n",
        "8. [Exemplos Completos](#exemplos-completos)\n",
        "9. [Usando help()](#usando-help)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Vis√£o Geral\n",
        "\n",
        "Este tutorial mostra como usar os tr√™s m√≥dulos Python para experimentos de Aprendizado por Refor√ßo:\n",
        "\n",
        "- **`environment.py`**: Cria√ß√£o e manipula√ß√£o de ambientes GridWorld\n",
        "- **`algorithms.py`**: Implementa√ß√µes de algoritmos de RL\n",
        "- **`visualization.py`**: Visualiza√ß√£o de resultados\n",
        "\n",
        "### Instala√ß√£o\n",
        "\n",
        "Coloque os tr√™s arquivos `.py` na mesma pasta do seu notebook Jupyter.\n",
        "\n",
        "### Importa√ß√£o B√°sica\n",
        "\n",
        "```python\n",
        "# Importar m√≥dulos\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importar classes e fun√ß√µes principais\n",
        "from environment import (\n",
        "    GridWorld,\n",
        "    create_classic_gridworld,\n",
        "    create_custom_gridworld,\n",
        "    print_gridworld_info\n",
        ")\n",
        "\n",
        "from algorithms import (\n",
        "    # Predi√ß√£o\n",
        "    td_zero_prediction,\n",
        "    first_visit_mc_prediction,\n",
        "    # Controle\n",
        "    sarsa,\n",
        "    q_learning,\n",
        "    expected_sarsa,\n",
        "    first_visit_mc_control,\n",
        "    mc_exploring_starts,\n",
        "    # Auxiliares\n",
        "    get_greedy_policy\n",
        ")\n",
        "\n",
        "from visualization import (\n",
        "    visualize_gridworld,\n",
        "    visualize_q_values,\n",
        "    visualize_q_table_detailed,\n",
        "    plot_learning_curves,\n",
        "    plot_value_evolution,\n",
        "    plot_value_heatmap,\n",
        "    plot_q_value_heatmap,\n",
        "    compare_algorithms,\n",
        "    print_q_table\n",
        ")\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6c9a7b",
      "metadata": {
        "id": "bf6c9a7b"
      },
      "source": [
        "# EXECUTE A C√âLULA ABAIXO CASO ESTEJA NO COLAB\n",
        "---\n",
        "CASO CONT√ÅRIO PULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a28101b",
      "metadata": {
        "id": "9a28101b"
      },
      "outputs": [],
      "source": [
        "# C√©lula 1: Upload dos arquivos Python\n",
        "try:\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"üì§ Fa√ßa upload dos 3 arquivos Python:\")\n",
        "    print(\"   1. environment.py\")\n",
        "    print(\"   2. algorithms.py\")\n",
        "    print(\"   3. visualization.py\")\n",
        "    print()\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    print(\"\\n‚úÖ Arquivos carregados com sucesso!\")\n",
        "    print(f\"   Total: {len(uploaded)} arquivo(s)\")\n",
        "except:\n",
        "    print('Caso esteja executando local baixe os arquivos do Github e passe para os imports')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08c033d",
      "metadata": {
        "id": "a08c033d"
      },
      "outputs": [],
      "source": [
        "# Importar m√≥dulos\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importar classes e fun√ß√µes principais\n",
        "from environment import (\n",
        "    GridWorld,\n",
        "    create_classic_gridworld,\n",
        "    create_custom_gridworld,\n",
        "    create_cliff_world,\n",
        "    create_cliff_world_2,\n",
        "    print_gridworld_info\n",
        ")\n",
        "\n",
        "from algorithms import (\n",
        "    # Predi√ß√£o\n",
        "    td_zero_prediction,\n",
        "    first_visit_mc_prediction,\n",
        "    # Controle\n",
        "    sarsa,\n",
        "    q_learning,\n",
        "    expected_sarsa,\n",
        "    first_visit_mc_control,\n",
        "    mc_exploring_starts,\n",
        "    # Auxiliares\n",
        "    get_greedy_policy\n",
        ")\n",
        "\n",
        "from visualization import (\n",
        "    visualize_gridworld,\n",
        "    visualize_q_values,\n",
        "    visualize_q_table_detailed,\n",
        "    plot_learning_curves,\n",
        "    plot_value_evolution,\n",
        "    plot_value_heatmap,\n",
        "    plot_q_value_heatmap,\n",
        "    compare_algorithms,\n",
        "    print_q_table\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53127b3",
      "metadata": {
        "id": "f53127b3"
      },
      "source": [
        "## 2. Estrutura dos M√≥dulos\n",
        "\n",
        "### üì¶ `environment.py`\n",
        "\n",
        "**Classe Principal:**\n",
        "- `GridWorld`: Ambiente de grid com transi√ß√µes estoc√°sticas\n",
        "\n",
        "**Fun√ß√µes de Cria√ß√£o:**\n",
        "- `create_classic_gridworld()`: Grid 4x3 cl√°ssico do Russell & Norvig\n",
        "- `create_custom_gridworld()`: Cria grid personalizado\n",
        "- `create_cliff_world()`: Ambiente Cliff World\n",
        "\n",
        "**Fun√ß√£o Auxiliar:**\n",
        "- `print_gridworld_info()`: Imprime informa√ß√µes do ambiente\n",
        "\n",
        "### üß† `algorithms.py`\n",
        "\n",
        "**Algoritmos de Predi√ß√£o:**\n",
        "- `td_zero_prediction()`: TD(0)\n",
        "- `first_visit_mc_prediction()`: Monte Carlo First-Visit\n",
        "\n",
        "**Algoritmos de Controle:**\n",
        "- `sarsa()`: SARSA (on-policy)\n",
        "- `q_learning()`: Q-Learning (off-policy)\n",
        "- `expected_sarsa()`: Expected SARSA\n",
        "- `first_visit_mc_control()`: MC Control\n",
        "- `mc_exploring_starts()`: MC Exploring Starts\n",
        "\n",
        "**Fun√ß√µes Auxiliares:**\n",
        "- `get_greedy_policy()`: Extrai pol√≠tica gulosa de Q\n",
        "- `epsilon_greedy_action()`: Escolhe a√ß√£o Œµ-greedy\n",
        "\n",
        "### üìä `visualization.py`\n",
        "\n",
        "**Visualiza√ß√£o de Grid:**\n",
        "- `visualize_gridworld()`: Visualiza grid com valores/pol√≠tica\n",
        "- `visualize_q_values()`: Mostra valores m√°ximos por estado\n",
        "- `visualize_q_table_detailed()`: Mostra Q(s,a) de todas as a√ß√µes\n",
        "\n",
        "**Curvas de Aprendizado:**\n",
        "- `plot_learning_curves()`: Plota curvas de aprendizado\n",
        "- `plot_value_evolution()`: Evolu√ß√£o de valores ao longo do tempo\n",
        "\n",
        "**Heatmaps:**\n",
        "- `plot_value_heatmap()`: Heatmap de valores V(s)\n",
        "- `plot_q_value_heatmap()`: Heatmap de Q-values\n",
        "\n",
        "**An√°lise:**\n",
        "- `compare_algorithms()`: Compara m√∫ltiplos algoritmos\n",
        "- `print_q_table()`: Imprime tabela Q formatada\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5875e59",
      "metadata": {
        "id": "e5875e59"
      },
      "source": [
        "## 3. Criando Ambientes GridWorld\n",
        "\n",
        "### 3.1 GridWorld Cl√°ssico 4x3\n",
        "\n",
        "```python\n",
        "# Criar ambiente cl√°ssico\n",
        "gw = create_classic_gridworld()\n",
        "\n",
        "# Visualizar\n",
        "visualize_gridworld(gw, title=\"GridWorld 4x3 Cl√°ssico\")\n",
        "\n",
        "# Imprimir informa√ß√µes\n",
        "print_gridworld_info(gw)\n",
        "```\n",
        "\n",
        "**Sa√≠da:**\n",
        "```\n",
        "======================================================================\n",
        "INFORMA√á√ïES DO GRIDWORLD\n",
        "======================================================================\n",
        "Dimens√µes: 3 linhas x 4 colunas\n",
        "Total de estados: 11\n",
        "Paredes: 1\n",
        "Estados terminais: 2\n",
        "Fator de desconto (Œ≥): 0.9\n",
        "Ru√≠do: 0.2\n",
        "Living reward: -0.04\n",
        "A√ß√µes dispon√≠veis: ['N', 'S', 'L', 'O']\n",
        "\n",
        "Paredes: [(1, 1)]\n",
        "\n",
        "Estados terminais:\n",
        "  (0, 3): reward = 1.0\n",
        "  (1, 3): reward = -1.0\n",
        "======================================================================\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c9a954",
      "metadata": {
        "id": "a4c9a954"
      },
      "outputs": [],
      "source": [
        "# Criar ambiente cl√°ssico\n",
        "gw = create_classic_gridworld()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f001c541",
      "metadata": {
        "id": "f001c541"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "visualize_gridworld(gw, title=\"GridWorld 4x3 Cl√°ssico\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d32efcd",
      "metadata": {
        "id": "8d32efcd"
      },
      "outputs": [],
      "source": [
        "# Imprimir informa√ß√µes\n",
        "print_gridworld_info(gw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c84b7c",
      "metadata": {
        "id": "e9c84b7c"
      },
      "source": [
        "### 3.2 GridWorld Personalizado\n",
        "\n",
        "```python\n",
        "# Criar grid 5x5 personalizado\n",
        "gw_custom = create_custom_gridworld(\n",
        "    rows=5,\n",
        "    cols=5,\n",
        "    walls=[(1, 1), (1, 2), (2, 1), (2, 2)],  # Bloco de paredes\n",
        "    terminals={(0, 4): 10.0, (4, 0): -10.0},  # Dois terminais\n",
        "    gamma=0.95,\n",
        "    noise=0.1,\n",
        "    living_reward=-0.1\n",
        ")\n",
        "\n",
        "visualize_gridworld(gw_custom, title=\"GridWorld Personalizado 5x5\")\n",
        "print_gridworld_info(gw_custom)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7379eb",
      "metadata": {
        "id": "9c7379eb"
      },
      "outputs": [],
      "source": [
        "# Criar grid 5x5 personalizado\n",
        "gw_custom = create_custom_gridworld(\n",
        "    rows=5,\n",
        "    cols=5,\n",
        "    walls=[(1, 1), (1, 2), (2, 1), (2, 2)],  # Bloco de paredes\n",
        "    terminals={(0, 4): 10.0, (4, 0): -10.0},  # Dois terminais\n",
        "    gamma=0.95,\n",
        "    noise=0.1,\n",
        "    living_reward=-0.1\n",
        ")\n",
        "\n",
        "visualize_gridworld(gw_custom, title=\"GridWorld Personalizado 5x5\")\n",
        "print_gridworld_info(gw_custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b61014a",
      "metadata": {
        "id": "3b61014a"
      },
      "source": [
        "### 3.3 Cliff World\n",
        "\n",
        "```python\n",
        "# Criar Cliff World\n",
        "gw_cliff = create_cliff_world()\n",
        "\n",
        "visualize_gridworld(gw_cliff, title=\"Cliff World\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113a3e1c",
      "metadata": {
        "id": "113a3e1c"
      },
      "outputs": [],
      "source": [
        "# Criar Cliff World\n",
        "gw_cliff = create_cliff_world()\n",
        "\n",
        "visualize_gridworld(gw_cliff, title=\"Cliff World\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaa281b",
      "metadata": {
        "id": "feaa281b"
      },
      "source": [
        "### 3.4 Criando Manualmente\n",
        "\n",
        "```python\n",
        "# Criar grid vazio\n",
        "gw_manual = GridWorld(rows=4, cols=6, gamma=0.9, noise=0.2)\n",
        "\n",
        "# Adicionar paredes\n",
        "gw_manual.set_wall(1, 2)\n",
        "gw_manual.set_wall(1, 3)\n",
        "gw_manual.set_wall(2, 2)\n",
        "\n",
        "# Adicionar terminais\n",
        "gw_manual.set_terminal(0, 5, 5.0)   # Goal\n",
        "gw_manual.set_terminal(3, 5, -5.0)  # Trap\n",
        "\n",
        "# Configurar living reward\n",
        "gw_manual.living_reward = -0.05\n",
        "\n",
        "visualize_gridworld(gw_manual, title=\"GridWorld Manual\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cd0eb7",
      "metadata": {
        "id": "58cd0eb7"
      },
      "outputs": [],
      "source": [
        "# Criar grid vazio\n",
        "gw_manual = GridWorld(rows=4, cols=6, gamma=0.9, noise=0.2)\n",
        "\n",
        "# Adicionar paredes\n",
        "gw_manual.set_wall(1, 2)\n",
        "gw_manual.set_wall(1, 3)\n",
        "gw_manual.set_wall(2, 2)\n",
        "\n",
        "# Adicionar terminais\n",
        "gw_manual.set_terminal(0, 5, 5.0)   # Goal\n",
        "gw_manual.set_terminal(3, 5, -5.0)  # Trap\n",
        "\n",
        "# Configurar living reward\n",
        "gw_manual.living_reward = -0.05\n",
        "\n",
        "visualize_gridworld(gw_manual, title=\"GridWorld Manual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9950462f",
      "metadata": {
        "id": "9950462f"
      },
      "source": [
        "## 4. Criando Pol√≠ticas\n",
        "\n",
        "### 4.1 Pol√≠tica Aleat√≥ria\n",
        "\n",
        "```python\n",
        "def create_random_policy(gridworld):\n",
        "    \"\"\"Cria pol√≠tica completamente aleat√≥ria.\"\"\"\n",
        "    policy = {}\n",
        "    for state in gridworld.states:\n",
        "        if not gridworld.is_terminal(state):\n",
        "            policy[state] = np.random.choice(gridworld.actions)\n",
        "    return policy\n",
        "\n",
        "# Usar\n",
        "policy_random = create_random_policy(gw)\n",
        "visualize_gridworld(gw, policy=policy_random, title=\"Pol√≠tica Aleat√≥ria\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3638926",
      "metadata": {
        "id": "e3638926"
      },
      "outputs": [],
      "source": [
        "def create_random_policy(gridworld):\n",
        "    \"\"\"Cria pol√≠tica completamente aleat√≥ria.\"\"\"\n",
        "    policy = {}\n",
        "    for state in gridworld.states:\n",
        "        if not gridworld.is_terminal(state):\n",
        "            policy[state] = np.random.choice(gridworld.actions)\n",
        "    return policy\n",
        "\n",
        "# Usar\n",
        "policy_random = create_random_policy(gw)\n",
        "visualize_gridworld(gw, policy=policy_random, title=\"Pol√≠tica Aleat√≥ria\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9119148a",
      "metadata": {
        "id": "9119148a"
      },
      "source": [
        "### 4.2 Pol√≠tica Fixa\n",
        "\n",
        "```python\n",
        "def create_fixed_policy(gridworld, action='N'):\n",
        "    \"\"\"Cria pol√≠tica que sempre escolhe a mesma a√ß√£o.\"\"\"\n",
        "    policy = {}\n",
        "    for state in gridworld.states:\n",
        "        if not gridworld.is_terminal(state):\n",
        "            policy[state] = action\n",
        "    return policy\n",
        "\n",
        "# Sempre ir para Norte\n",
        "policy_north = create_fixed_policy(gw, 'N')\n",
        "visualize_gridworld(gw, policy=policy_north, title=\"Pol√≠tica: Sempre Norte\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121967cb",
      "metadata": {
        "id": "121967cb"
      },
      "outputs": [],
      "source": [
        "def create_fixed_policy(gridworld, action='N'):\n",
        "    \"\"\"Cria pol√≠tica que sempre escolhe a mesma a√ß√£o.\"\"\"\n",
        "    policy = {}\n",
        "    for state in gridworld.states:\n",
        "        if not gridworld.is_terminal(state):\n",
        "            policy[state] = action\n",
        "    return policy\n",
        "\n",
        "# Sempre ir para Norte\n",
        "policy_north = create_fixed_policy(gw, 'N')\n",
        "visualize_gridworld(gw, policy=policy_north, title=\"Pol√≠tica: Sempre Norte\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cbbd707",
      "metadata": {
        "id": "3cbbd707"
      },
      "source": [
        "### 4.3 Pol√≠tica Customizada\n",
        "\n",
        "```python\n",
        "def create_custom_policy(gridworld):\n",
        "    \"\"\"Cria pol√≠tica espec√≠fica para o problema.\"\"\"\n",
        "    policy = {}\n",
        "    \n",
        "    # Exemplo: estrat√©gia para alcan√ßar (0,3)\n",
        "    policy[(0, 0)] = 'L'  # Ir para direita\n",
        "    policy[(0, 1)] = 'L'\n",
        "    policy[(0, 2)] = 'L'\n",
        "    policy[(1, 0)] = 'N'  # Subir\n",
        "    policy[(1, 2)] = 'N'\n",
        "    policy[(2, 0)] = 'N'\n",
        "    policy[(2, 1)] = 'L'\n",
        "    policy[(2, 2)] = 'L'\n",
        "    policy[(2, 3)] = 'N'\n",
        "    \n",
        "    return policy\n",
        "\n",
        "policy_custom = create_custom_policy(gw)\n",
        "visualize_gridworld(gw, policy=policy_custom, title=\"Pol√≠tica Customizada\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfa261b",
      "metadata": {
        "id": "9bfa261b"
      },
      "outputs": [],
      "source": [
        "def create_custom_policy(gridworld):\n",
        "    \"\"\"Cria pol√≠tica espec√≠fica para o problema.\"\"\"\n",
        "    policy = {}\n",
        "\n",
        "    # Exemplo: estrat√©gia para alcan√ßar (0,3)\n",
        "    policy[(0, 0)] = 'L'  # Ir para direita\n",
        "    policy[(0, 1)] = 'L'\n",
        "    policy[(0, 2)] = 'L'\n",
        "    policy[(1, 0)] = 'N'  # Subir\n",
        "    policy[(1, 2)] = 'N'\n",
        "    policy[(2, 0)] = 'N'\n",
        "    policy[(2, 1)] = 'L'\n",
        "    policy[(2, 2)] = 'L'\n",
        "    policy[(2, 3)] = 'N'\n",
        "\n",
        "    return policy\n",
        "\n",
        "policy_custom = create_custom_policy(gw)\n",
        "visualize_gridworld(gw, policy=policy_custom, title=\"Pol√≠tica Customizada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb908ed",
      "metadata": {
        "id": "9fb908ed"
      },
      "source": [
        "### 4.4 Pol√≠tica Gulosa de Q-values\n",
        "\n",
        "```python\n",
        "# Ap√≥s treinar um algoritmo de controle\n",
        "Q, _ = q_learning(gw, n_episodes=1000)\n",
        "\n",
        "# Extrair pol√≠tica gulosa\n",
        "policy_greedy = get_greedy_policy(Q, gw)\n",
        "\n",
        "visualize_gridworld(gw, policy=policy_greedy, title=\"Pol√≠tica Gulosa (Q-Learning)\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79927f87",
      "metadata": {
        "id": "79927f87"
      },
      "outputs": [],
      "source": [
        "# Ap√≥s treinar um algoritmo de controle\n",
        "Q, _ = q_learning(gw, n_episodes=1000)\n",
        "\n",
        "# Extrair pol√≠tica gulosa\n",
        "policy_greedy = get_greedy_policy(Q, gw)\n",
        "\n",
        "visualize_gridworld(gw, policy=policy_greedy, title=\"Pol√≠tica Gulosa (Q-Learning)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa4d7d6",
      "metadata": {
        "id": "caa4d7d6"
      },
      "source": [
        "## 5. Algoritmos de Predi√ß√£o\n",
        "\n",
        "### 5.1 TD(0) - Temporal Difference\n",
        "\n",
        "```python\n",
        "# Criar ambiente e pol√≠tica\n",
        "gw = create_classic_gridworld()\n",
        "policy = create_fixed_policy(gw, 'N')\n",
        "\n",
        "# Executar TD(0)\n",
        "V_td = td_zero_prediction(\n",
        "    gridworld=gw,\n",
        "    policy=policy,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar resultados\n",
        "visualize_gridworld(gw, values=V_td, title=\"TD(0) - Valores V(s)\")\n",
        "plot_value_heatmap(V_td, gw, title=\"TD(0) - Heatmap\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd65b50e",
      "metadata": {
        "id": "dd65b50e"
      },
      "outputs": [],
      "source": [
        "# Criar ambiente e pol√≠tica\n",
        "gw = create_classic_gridworld()\n",
        "policy = create_fixed_policy(gw, 'N')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edb42a4",
      "metadata": {
        "id": "1edb42a4"
      },
      "outputs": [],
      "source": [
        "# Executar TD(0)\n",
        "V_td = td_zero_prediction(\n",
        "    gridworld=gw,\n",
        "    policy=policy,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6297c17f",
      "metadata": {
        "id": "6297c17f"
      },
      "outputs": [],
      "source": [
        "# Visualizar resultados\n",
        "visualize_gridworld(gw, values=V_td,policy=policy, title=\"TD(0) - Valores V(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320b69e6",
      "metadata": {
        "id": "320b69e6"
      },
      "outputs": [],
      "source": [
        "plot_value_heatmap(V_td, gw, title=\"TD(0) - Heatmap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c13dcd9",
      "metadata": {
        "id": "9c13dcd9"
      },
      "source": [
        "### 5.2 Monte Carlo First-Visit\n",
        "\n",
        "```python\n",
        "# Executar Monte Carlo\n",
        "V_mc = first_visit_mc_prediction(\n",
        "    gridworld=gw,\n",
        "    policy=policy,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar\n",
        "visualize_gridworld(gw, values=V_mc, title=\"MC - Valores V(s)\")\n",
        "plot_value_heatmap(V_mc, gw, title=\"Monte Carlo - Heatmap\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c30117a",
      "metadata": {
        "id": "7c30117a"
      },
      "outputs": [],
      "source": [
        "# Executar Monte Carlo\n",
        "V_mc = first_visit_mc_prediction(\n",
        "    gridworld=gw,\n",
        "    policy=policy,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6854044a",
      "metadata": {
        "id": "6854044a"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "visualize_gridworld(gw, values=V_mc,policy=policy, title=\"MC - Valores V(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5f79e1",
      "metadata": {
        "id": "8e5f79e1"
      },
      "outputs": [],
      "source": [
        "plot_value_heatmap(V_mc, gw, title=\"Monte Carlo - Heatmap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c91e046",
      "metadata": {
        "id": "9c91e046"
      },
      "source": [
        "### 5.3 Comparando TD(0) vs Monte Carlo\n",
        "\n",
        "```python\n",
        "# Treinar ambos\n",
        "V_td = td_zero_prediction(gw, policy, n_episodes=1000, alpha=0.1)\n",
        "V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000, alpha=0.1)\n",
        "\n",
        "# Comparar valores\n",
        "print(\"Compara√ß√£o TD(0) vs Monte Carlo:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Estado':<15} {'TD(0)':<15} {'MC':<15} {'Diferen√ßa':<15}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for state in gw.states:\n",
        "    if not gw.is_terminal(state) and state not in gw.walls:\n",
        "        diff = abs(V_td[state] - V_mc[state])\n",
        "        print(f\"{str(state):<15} {V_td[state]:<15.4f} {V_mc[state]:<15.4f} {diff:<15.4f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09672795",
      "metadata": {
        "id": "09672795"
      },
      "outputs": [],
      "source": [
        "# Treinar ambos\n",
        "V_td = td_zero_prediction(gw, policy, n_episodes=1000, alpha=0.1)\n",
        "V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000, alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4003ca6b",
      "metadata": {
        "id": "4003ca6b"
      },
      "outputs": [],
      "source": [
        "# Comparar valores\n",
        "print(\"Compara√ß√£o TD(0) vs Monte Carlo:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Estado':<15} {'TD(0)':<15} {'MC':<15} {'Diferen√ßa':<15}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for state in gw.states:\n",
        "    if not gw.is_terminal(state) and state not in gw.walls:\n",
        "        diff = abs(V_td[state] - V_mc[state])\n",
        "        print(f\"{str(state):<15} {V_td[state]:<15.4f} {V_mc[state]:<15.4f} {diff:<15.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0070b00f",
      "metadata": {
        "id": "0070b00f"
      },
      "source": [
        "### 5.4 Monitorando Evolu√ß√£o dos Valores\n",
        "\n",
        "```python\n",
        "# Salvar valores periodicamente\n",
        "value_history = []\n",
        "policy = create_fixed_policy(gw, 'L')\n",
        "\n",
        "# Treinar salvando valores a cada 100 epis√≥dios\n",
        "for checkpoint in range(0, 1000, 100):\n",
        "    V = td_zero_prediction(gw, policy, n_episodes=100, alpha=0.1)\n",
        "    value_history.append(V.copy())\n",
        "\n",
        "# Plotar evolu√ß√£o\n",
        "states_to_monitor = [(0, 0), (1, 0), (2, 0), (0, 2)]\n",
        "plot_value_evolution(value_history, states_to_monitor,\n",
        "                    title=\"Evolu√ß√£o de V(s) ao longo do treinamento\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc881e40",
      "metadata": {
        "id": "fc881e40"
      },
      "outputs": [],
      "source": [
        "# Salvar valores periodicamente\n",
        "value_history = []\n",
        "policy = create_fixed_policy(gw, 'L')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e22496a8",
      "metadata": {
        "id": "e22496a8"
      },
      "outputs": [],
      "source": [
        "# Treinar salvando valores a cada 100 epis√≥dios\n",
        "for checkpoint in range(0, 1000, 100):\n",
        "    V = td_zero_prediction(gw, policy, n_episodes=100, alpha=0.1)\n",
        "    value_history.append(V.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0820519",
      "metadata": {
        "id": "c0820519"
      },
      "outputs": [],
      "source": [
        "# Plotar evolu√ß√£o\n",
        "states_to_monitor = [(0, 0), (1, 0), (2, 0), (0, 2)]\n",
        "plot_value_evolution(value_history, states_to_monitor,\n",
        "                    title=\"Evolu√ß√£o de V(s) ao longo do treinamento\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4891c5e9",
      "metadata": {
        "id": "4891c5e9"
      },
      "source": [
        "## 6. Algoritmos de Controle\n",
        "\n",
        "### 6.1 SARSA (On-Policy)\n",
        "\n",
        "```python\n",
        "# Treinar SARSA\n",
        "Q_sarsa, rewards_sarsa = sarsa(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Extrair pol√≠tica\n",
        "policy_sarsa = get_greedy_policy(Q_sarsa, gw)\n",
        "\n",
        "# Visualizar\n",
        "visualize_gridworld(gw, policy=policy_sarsa, title=\"SARSA - Pol√≠tica Aprendida\")\n",
        "visualize_q_values(Q_sarsa, gw, title=\"SARSA - Valores Q\")\n",
        "visualize_q_table_detailed(Q_sarsa, gw, title=\"SARSA - Q-Values Detalhados\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156f9b42",
      "metadata": {
        "id": "156f9b42"
      },
      "outputs": [],
      "source": [
        "# Treinar SARSA\n",
        "Q_sarsa, rewards_sarsa = sarsa(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d64621",
      "metadata": {
        "id": "d3d64621"
      },
      "outputs": [],
      "source": [
        "# Extrair pol√≠tica\n",
        "policy_sarsa = get_greedy_policy(Q_sarsa, gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ae4502",
      "metadata": {
        "id": "66ae4502"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "visualize_gridworld(gw, policy=policy_sarsa, title=\"SARSA - Pol√≠tica Aprendida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9bbef03",
      "metadata": {
        "id": "a9bbef03"
      },
      "outputs": [],
      "source": [
        "visualize_q_values(Q_sarsa, gw, title=\"SARSA - Valores Q\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c07172",
      "metadata": {
        "id": "34c07172"
      },
      "outputs": [],
      "source": [
        "visualize_q_table_detailed(Q_sarsa, gw, title=\"SARSA - Q-Values Detalhados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee10d9b",
      "metadata": {
        "id": "4ee10d9b"
      },
      "source": [
        "### 6.2 Q-Learning (Off-Policy)\n",
        "\n",
        "```python\n",
        "# Treinar Q-Learning\n",
        "Q_qlearning, rewards_qlearning = q_learning(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar\n",
        "policy_qlearning = get_greedy_policy(Q_qlearning, gw)\n",
        "visualize_gridworld(gw, policy=policy_qlearning, title=\"Q-Learning - Pol√≠tica\")\n",
        "visualize_q_values(Q_qlearning, gw, title=\"Q-Learning - Valores Q\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4826c35a",
      "metadata": {
        "id": "4826c35a"
      },
      "outputs": [],
      "source": [
        "# Treinar Q-Learning\n",
        "Q_qlearning, rewards_qlearning = q_learning(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade55553",
      "metadata": {
        "id": "ade55553"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "policy_qlearning = get_greedy_policy(Q_qlearning, gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff60804c",
      "metadata": {
        "id": "ff60804c"
      },
      "outputs": [],
      "source": [
        "visualize_gridworld(gw, policy=policy_qlearning, title=\"Q-Learning - Pol√≠tica\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1ccc4f",
      "metadata": {
        "id": "9e1ccc4f"
      },
      "outputs": [],
      "source": [
        "visualize_q_values(Q_qlearning, gw, title=\"Q-Learning - Valores Q\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d1f08f",
      "metadata": {
        "id": "89d1f08f"
      },
      "outputs": [],
      "source": [
        "visualize_q_table_detailed(Q_qlearning, gw, title=\"Q-Learning - Q-Values Detalhados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a429fe2a",
      "metadata": {
        "id": "a429fe2a"
      },
      "source": [
        "### 6.3 Expected SARSA\n",
        "\n",
        "```python\n",
        "# Treinar Expected SARSA\n",
        "Q_expected, rewards_expected = expected_sarsa(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar\n",
        "policy_expected = get_greedy_policy(Q_expected, gw)\n",
        "visualize_gridworld(gw, policy=policy_expected, title=\"Expected SARSA - Pol√≠tica\")\n",
        "visualize_q_values(Q_expected, gw, title=\"Expected SARSA - Valores Q\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720a069f",
      "metadata": {
        "id": "720a069f"
      },
      "outputs": [],
      "source": [
        "# Treinar Expected SARSA\n",
        "Q_expected, rewards_expected = expected_sarsa(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57a2362",
      "metadata": {
        "id": "a57a2362"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "policy_expected = get_greedy_policy(Q_expected, gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2401640c",
      "metadata": {
        "id": "2401640c"
      },
      "outputs": [],
      "source": [
        "visualize_gridworld(gw, policy=policy_expected, title=\"Expected SARSA - Pol√≠tica\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af54b8e1",
      "metadata": {
        "id": "af54b8e1"
      },
      "outputs": [],
      "source": [
        "visualize_q_values(Q_expected, gw, title=\"Expected SARSA - Valores Q\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d881a07d",
      "metadata": {
        "id": "d881a07d"
      },
      "outputs": [],
      "source": [
        "visualize_q_table_detailed(Q_expected, gw, title=\"Expected Sarsa- Q-Values Detalhados\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912fccf9",
      "metadata": {
        "id": "912fccf9"
      },
      "source": [
        "### 6.4 Monte Carlo Control\n",
        "\n",
        "```python\n",
        "# Treinar MC Control\n",
        "Q_mc, rewards_mc = first_visit_mc_control(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar\n",
        "policy_mc = get_greedy_policy(Q_mc, gw)\n",
        "visualize_gridworld(gw, policy=policy_mc, title=\"MC Control - Pol√≠tica\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a0d895",
      "metadata": {
        "id": "74a0d895"
      },
      "outputs": [],
      "source": [
        "# Treinar MC Control\n",
        "Q_mc, rewards_mc = first_visit_mc_control(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae74347",
      "metadata": {
        "id": "9ae74347"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "policy_mc = get_greedy_policy(Q_mc, gw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a9db27",
      "metadata": {
        "id": "97a9db27"
      },
      "outputs": [],
      "source": [
        "visualize_gridworld(gw, policy=policy_mc, title=\"MC Control - Pol√≠tica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c258086f",
      "metadata": {
        "id": "c258086f"
      },
      "source": [
        "### 6.5 Monte Carlo Exploring Starts\n",
        "\n",
        "```python\n",
        "# Treinar MC ES\n",
        "Q_es, rewards_es = mc_exploring_starts(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    max_steps=500,  # Importante: evita loops infinitos\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar\n",
        "policy_es = get_greedy_policy(Q_es, gw)\n",
        "visualize_gridworld(gw, policy=policy_es, title=\"MC Exploring Starts - Pol√≠tica\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce8ea5c",
      "metadata": {
        "id": "cce8ea5c"
      },
      "outputs": [],
      "source": [
        "# Treinar MC ES\n",
        "Q_es, rewards_es = mc_exploring_starts(\n",
        "    gridworld=gw,\n",
        "    n_episodes=1000,\n",
        "    alpha=0.1,\n",
        "    max_steps=500,  # Importante: evita loops infinitos\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c365bc0",
      "metadata": {
        "id": "5c365bc0"
      },
      "outputs": [],
      "source": [
        "# Visualizar\n",
        "policy_es = get_greedy_policy(Q_es, gw)\n",
        "visualize_gridworld(gw, policy=policy_es, title=\"MC Exploring Starts - Pol√≠tica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b8a5e9",
      "metadata": {
        "id": "49b8a5e9"
      },
      "source": [
        "## 7. Visualiza√ß√£o de Resultados\n",
        "\n",
        "### 7.1 Visualizar Grid B√°sico\n",
        "\n",
        "```python\n",
        "# Apenas o grid\n",
        "visualize_gridworld(gw, title=\"GridWorld Vazio\")\n",
        "\n",
        "# Grid com valores\n",
        "visualize_gridworld(gw, values=V_td, title=\"Com Valores V(s)\")\n",
        "\n",
        "# Grid com pol√≠tica\n",
        "visualize_gridworld(gw, policy=policy_sarsa, title=\"Com Pol√≠tica\")\n",
        "\n",
        "# Grid com valores E pol√≠tica\n",
        "visualize_gridworld(gw, values=V_td, policy=policy_sarsa,\n",
        "                   title=\"Valores + Pol√≠tica\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9150d6a1",
      "metadata": {
        "id": "9150d6a1"
      },
      "outputs": [],
      "source": [
        "# Apenas o grid\n",
        "visualize_gridworld(gw, title=\"GridWorld Vazio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d77c247",
      "metadata": {
        "id": "7d77c247"
      },
      "outputs": [],
      "source": [
        "# Grid com valores\n",
        "visualize_gridworld(gw, values=V_td, title=\"Com Valores V(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ca3b90a",
      "metadata": {
        "id": "6ca3b90a"
      },
      "outputs": [],
      "source": [
        "# Grid com pol√≠tica\n",
        "visualize_gridworld(gw, policy=policy_sarsa, title=\"Com Pol√≠tica\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d7ae7d",
      "metadata": {
        "id": "c7d7ae7d"
      },
      "outputs": [],
      "source": [
        "# Grid com valores E pol√≠tica\n",
        "visualize_gridworld(gw, values=V_td, policy=policy_sarsa,\n",
        "                   title=\"Valores + Pol√≠tica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e16c153",
      "metadata": {
        "id": "9e16c153"
      },
      "source": [
        "### 7.2 Visualizar Q-Values\n",
        "\n",
        "```python\n",
        "# Valores m√°ximos por estado\n",
        "visualize_q_values(Q_sarsa, gw, title=\"SARSA - Max Q-Values\")\n",
        "\n",
        "# Q-values detalhados (todas as a√ß√µes)\n",
        "visualize_q_table_detailed(Q_sarsa, gw, title=\"SARSA - Q(s,a) Detalhado\")\n",
        "\n",
        "# Imprimir tabela Q no console\n",
        "print_q_table(Q_sarsa, gw, title=\"SARSA - Tabela Q Completa\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82add987",
      "metadata": {
        "id": "82add987"
      },
      "outputs": [],
      "source": [
        "# Valores m√°ximos por estado\n",
        "visualize_q_values(Q_sarsa, gw, title=\"SARSA - Max Q-Values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27f1480d",
      "metadata": {
        "id": "27f1480d"
      },
      "outputs": [],
      "source": [
        "# Q-values detalhados (todas as a√ß√µes)\n",
        "visualize_q_table_detailed(Q_sarsa, gw, title=\"SARSA - Q(s,a) Detalhado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a952c24",
      "metadata": {
        "id": "6a952c24"
      },
      "outputs": [],
      "source": [
        "# Imprimir tabela Q no console\n",
        "print_q_table(Q_sarsa, gw, title=\"SARSA - Tabela Q Completa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9576f82",
      "metadata": {
        "id": "c9576f82"
      },
      "source": [
        "### 7.3 Curvas de Aprendizado\n",
        "\n",
        "```python\n",
        "# Treinar m√∫ltiplos algoritmos\n",
        "Q_s, rewards_s = sarsa(gw, n_episodes=1000)\n",
        "Q_q, rewards_q = q_learning(gw, n_episodes=1000)\n",
        "Q_e, rewards_e = expected_sarsa(gw, n_episodes=1000)\n",
        "\n",
        "# Plotar curvas\n",
        "plot_learning_curves({\n",
        "    'SARSA': rewards_s,\n",
        "    'Q-Learning': rewards_q,\n",
        "    'Expected SARSA': rewards_e\n",
        "}, window=100, title=\"Compara√ß√£o de Algoritmos\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8f86e2",
      "metadata": {
        "id": "df8f86e2"
      },
      "outputs": [],
      "source": [
        "# Treinar m√∫ltiplos algoritmos\n",
        "Q_s, rewards_s = sarsa(gw, n_episodes=1000)\n",
        "Q_q, rewards_q = q_learning(gw, n_episodes=1000)\n",
        "Q_e, rewards_e = expected_sarsa(gw, n_episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc8971c8",
      "metadata": {
        "id": "fc8971c8"
      },
      "outputs": [],
      "source": [
        "# Plotar curvas\n",
        "plot_learning_curves({\n",
        "    'SARSA': rewards_s,\n",
        "    'Q-Learning': rewards_q,\n",
        "    'Expected SARSA': rewards_e\n",
        "}, window=100, title=\"Compara√ß√£o de Algoritmos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2735bc6e",
      "metadata": {
        "id": "2735bc6e"
      },
      "source": [
        "### 7.4 Heatmaps de Valores\n",
        "\n",
        "```python\n",
        "# Heatmap de V(s) - Predi√ß√£o\n",
        "plot_value_heatmap(V_td, gw, title=\"TD(0) - Heatmap de Valores\")\n",
        "\n",
        "# Heatmap de max Q(s,a) - Controle\n",
        "plot_q_value_heatmap(Q_qlearning, gw, title=\"Q-Learning - Max Q-Values\")\n",
        "\n",
        "# Heatmap para a√ß√£o espec√≠fica\n",
        "plot_q_value_heatmap(Q_qlearning, gw, action='N',\n",
        "                    title=\"Q-Learning - Q(s, Norte)\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a6ebf8",
      "metadata": {
        "id": "24a6ebf8"
      },
      "outputs": [],
      "source": [
        "# Heatmap de V(s) - Predi√ß√£o\n",
        "plot_value_heatmap(V_td, gw, title=\"TD(0) - Heatmap de Valores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881d1d97",
      "metadata": {
        "id": "881d1d97"
      },
      "outputs": [],
      "source": [
        "# Heatmap de max Q(s,a) - Controle\n",
        "plot_q_value_heatmap(Q_qlearning, gw, title=\"Q-Learning - Max Q-Values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b11ced2",
      "metadata": {
        "id": "7b11ced2"
      },
      "outputs": [],
      "source": [
        "# Heatmap para a√ß√£o espec√≠fica\n",
        "plot_q_value_heatmap(Q_qlearning, gw, action='N',\n",
        "                    title=\"Q-Learning - Q(s, Norte)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dac7128",
      "metadata": {
        "id": "4dac7128"
      },
      "outputs": [],
      "source": [
        "# Heatmap para a√ß√£o espec√≠fica\n",
        "plot_q_value_heatmap(Q_qlearning, gw, action='L',\n",
        "                    title=\"Q-Learning - Q(s, Leste)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbe3460",
      "metadata": {
        "id": "fcbe3460"
      },
      "outputs": [],
      "source": [
        "# Heatmap para a√ß√£o espec√≠fica\n",
        "plot_q_value_heatmap(Q_qlearning, gw, action='S',\n",
        "                    title=\"Q-Learning - Q(s, Sul)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9de961e",
      "metadata": {
        "id": "f9de961e"
      },
      "outputs": [],
      "source": [
        "# Heatmap para a√ß√£o espec√≠fica\n",
        "plot_q_value_heatmap(Q_qlearning, gw, action='O',\n",
        "                    title=\"Q-Learning - Q(s, Oeste)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e7ab39",
      "metadata": {
        "id": "22e7ab39"
      },
      "source": [
        "### 7.5 Comparar Algoritmos\n",
        "\n",
        "```python\n",
        "# Comparar valores aprendidos\n",
        "compare_algorithms({\n",
        "    'SARSA': Q_sarsa,\n",
        "    'Q-Learning': Q_qlearning,\n",
        "    'Expected SARSA': Q_expected\n",
        "}, gw)\n",
        "```\n",
        "\n",
        "**Sa√≠da:**\n",
        "```\n",
        "================================================================================\n",
        "COMPARA√á√ÉO DE VALORES APRENDIDOS\n",
        "================================================================================\n",
        "\n",
        "Estado         SARSA               Q-Learning          Expected SARSA      \n",
        "--------------------------------------------------------------------------------\n",
        "(0, 0)         0.8123              0.8956              0.8542              \n",
        "(0, 1)         0.9012              0.9234              0.9087              \n",
        "...\n",
        "\n",
        "ESTAT√çSTICAS RESUMIDAS:\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "SARSA:\n",
        "  Valor m√©dio:   0.4523\n",
        "  Valor m√°ximo:  0.9456\n",
        "  Valor m√≠nimo:  -0.5234\n",
        "  Desvio padr√£o: 0.3421\n",
        "\n",
        "Q-Learning:\n",
        "  Valor m√©dio:   0.5012\n",
        "  Valor m√°ximo:  0.9678\n",
        "  Valor m√≠nimo:  -0.4123\n",
        "  Desvio padr√£o: 0.3256\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e9a674",
      "metadata": {
        "id": "34e9a674"
      },
      "outputs": [],
      "source": [
        "# Comparar valores aprendidos\n",
        "compare_algorithms({\n",
        "    'SARSA': Q_sarsa,\n",
        "    'Q-Learning': Q_qlearning,\n",
        "    'Expected SARSA': Q_expected\n",
        "}, gw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4326b85",
      "metadata": {
        "id": "c4326b85"
      },
      "source": [
        "## 8. Exemplos Completos\n",
        "\n",
        "### 8.1 Experimento Completo: Compara√ß√£o de Algoritmos TD\n",
        "\n",
        "```python\n",
        "# ============================================\n",
        "# CONFIGURA√á√ÉO\n",
        "# ============================================\n",
        "from environment import create_classic_gridworld, print_gridworld_info\n",
        "from algorithms import sarsa, q_learning, expected_sarsa\n",
        "from visualization import (\n",
        "    visualize_gridworld,\n",
        "    visualize_q_values,\n",
        "    plot_learning_curves,\n",
        "    compare_algorithms\n",
        ")\n",
        "\n",
        "# Criar ambiente\n",
        "gw = create_classic_gridworld()\n",
        "print_gridworld_info(gw)\n",
        "visualize_gridworld(gw, title=\"Ambiente: GridWorld 4x3\")\n",
        "\n",
        "# Par√¢metros\n",
        "PARAMS = {\n",
        "    'n_episodes': 1000,\n",
        "    'alpha': 0.1,\n",
        "    'gamma': 0.9,\n",
        "    'epsilon': 0.1\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# TREINAMENTO\n",
        "# ============================================\n",
        "print(\"\\n‚Üí Treinando SARSA...\")\n",
        "Q_sarsa, rewards_sarsa = sarsa(gw, **PARAMS, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Q-Learning...\")\n",
        "Q_qlearning, rewards_qlearning = q_learning(gw, **PARAMS, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Expected SARSA...\")\n",
        "Q_expected, rewards_expected = expected_sarsa(gw, **PARAMS, verbose=True)\n",
        "\n",
        "# ============================================\n",
        "# VISUALIZA√á√ÉO\n",
        "# ============================================\n",
        "# Curvas de aprendizado\n",
        "plot_learning_curves({\n",
        "    'SARSA': rewards_sarsa,\n",
        "    'Q-Learning': rewards_qlearning,\n",
        "    'Expected SARSA': rewards_expected\n",
        "}, title=\"Compara√ß√£o de Converg√™ncia\")\n",
        "\n",
        "# Pol√≠ticas aprendidas\n",
        "for name, Q in [('SARSA', Q_sarsa), ('Q-Learning', Q_qlearning),\n",
        "                ('Expected SARSA', Q_expected)]:\n",
        "    policy = get_greedy_policy(Q, gw)\n",
        "    visualize_gridworld(gw, policy=policy, title=f\"{name} - Pol√≠tica\")\n",
        "    visualize_q_values(Q, gw, title=f\"{name} - Valores Q\")\n",
        "\n",
        "# Compara√ß√£o num√©rica\n",
        "compare_algorithms({\n",
        "    'SARSA': Q_sarsa,\n",
        "    'Q-Learning': Q_qlearning,\n",
        "    'Expected SARSA': Q_expected\n",
        "}, gw)\n",
        "\n",
        "print(\"\\n‚úì Experimento conclu√≠do!\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf6240d",
      "metadata": {
        "id": "abf6240d"
      },
      "outputs": [],
      "source": [
        "# Criar ambiente\n",
        "gw = create_classic_gridworld()\n",
        "print_gridworld_info(gw)\n",
        "visualize_gridworld(gw, title=\"Ambiente: GridWorld 4x3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12f8cc9",
      "metadata": {
        "id": "d12f8cc9"
      },
      "outputs": [],
      "source": [
        "# Par√¢metros\n",
        "PARAMS = {\n",
        "    'n_episodes': 1000,\n",
        "    'alpha': 0.1,\n",
        "    'gamma': 0.9,\n",
        "    'epsilon': 0.1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808a5244",
      "metadata": {
        "id": "808a5244"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TREINAMENTO\n",
        "# ============================================\n",
        "print(\"\\n‚Üí Treinando SARSA...\")\n",
        "Q_sarsa, rewards_sarsa = sarsa(gw, **PARAMS, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Q-Learning...\")\n",
        "Q_qlearning, rewards_qlearning = q_learning(gw, **PARAMS, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Expected SARSA...\")\n",
        "Q_expected, rewards_expected = expected_sarsa(gw, **PARAMS, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c68ea1",
      "metadata": {
        "id": "55c68ea1"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# VISUALIZA√á√ÉO\n",
        "# ============================================\n",
        "# Curvas de aprendizado\n",
        "plot_learning_curves({\n",
        "    'SARSA': rewards_sarsa,\n",
        "    'Q-Learning': rewards_qlearning,\n",
        "    'Expected SARSA': rewards_expected\n",
        "}, title=\"Compara√ß√£o de Converg√™ncia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e299cd",
      "metadata": {
        "id": "d3e299cd"
      },
      "outputs": [],
      "source": [
        "# Pol√≠ticas aprendidas\n",
        "for name, Q in [('SARSA', Q_sarsa), ('Q-Learning', Q_qlearning),\n",
        "                ('Expected SARSA', Q_expected)]:\n",
        "    policy = get_greedy_policy(Q, gw)\n",
        "    visualize_gridworld(gw, policy=policy, title=f\"{name} - Pol√≠tica\")\n",
        "    visualize_q_values(Q, gw, title=f\"{name} - Valores Q\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ab0cf8",
      "metadata": {
        "id": "a2ab0cf8"
      },
      "outputs": [],
      "source": [
        "# Compara√ß√£o num√©rica\n",
        "compare_algorithms({\n",
        "    'SARSA': Q_sarsa,\n",
        "    'Q-Learning': Q_qlearning,\n",
        "    'Expected SARSA': Q_expected\n",
        "}, gw)\n",
        "\n",
        "print(\"\\n‚úì Experimento conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4fefbd",
      "metadata": {
        "id": "1b4fefbd"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURA√á√ÉO\n",
        "# ============================================\n",
        "from environment import create_cliff_world, print_gridworld_info\n",
        "from algorithms import sarsa, q_learning, expected_sarsa\n",
        "from visualization import (\n",
        "    visualize_gridworld,\n",
        "    visualize_q_values,\n",
        "    plot_learning_curves,\n",
        "    compare_algorithms\n",
        ")\n",
        "\n",
        "# Criar ambiente\n",
        "cw = create_cliff_world()\n",
        "#cw.set_terminal(3, 7, 50.0)\n",
        "#cw.set_terminal(0, 7, -1.0)\n",
        "print_gridworld_info(cw)\n",
        "visualize_gridworld(cw, title=\"Ambiente: Cliff World\")\n",
        "\n",
        "# Par√¢metros\n",
        "PARAMS = {\n",
        "    'n_episodes': 500,\n",
        "    'alpha': 0.1,\n",
        "    'gamma': 0.9,\n",
        "    'epsilon': 0.1\n",
        "}\n",
        "\n",
        "# ============================================\n",
        "# TREINAMENTO\n",
        "# ============================================\n",
        "print(\"\\n‚Üí Treinando SARSA...\")\n",
        "Q_sarsa, rewards_sarsa = sarsa(cw, **PARAMS, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Q-Learning...\")\n",
        "Q_qlearning, rewards_qlearning = q_learning(cw, **PARAMS, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Expected SARSA...\")\n",
        "Q_expected, rewards_expected = expected_sarsa(cw, **PARAMS, verbose=True)\n",
        "\n",
        "# ============================================\n",
        "# VISUALIZA√á√ÉO\n",
        "# ============================================\n",
        "# Curvas de aprendizado\n",
        "plot_learning_curves({\n",
        "    'SARSA': rewards_sarsa,\n",
        "    'Q-Learning': rewards_qlearning,\n",
        "    'Expected SARSA': rewards_expected\n",
        "}, title=\"Compara√ß√£o de Converg√™ncia\")\n",
        "\n",
        "# Pol√≠ticas aprendidas\n",
        "for name, Q in [('SARSA', Q_sarsa), ('Q-Learning', Q_qlearning),\n",
        "                ('Expected SARSA', Q_expected)]:\n",
        "    policy = get_greedy_policy(Q, cw)\n",
        "    visualize_gridworld(cw, policy=policy, title=f\"{name} - Pol√≠tica\")\n",
        "    visualize_q_values(Q, cw, title=f\"{name} - Valores Q\")\n",
        "    #visualize_q_table_detailed(Q, cw, title=f\"{name} - Valores Q Detalhados\")\n",
        "\n",
        "# Compara√ß√£o num√©rica\n",
        "compare_algorithms({\n",
        "    'SARSA': Q_sarsa,\n",
        "    'Q-Learning': Q_qlearning,\n",
        "    'Expected SARSA': Q_expected\n",
        "}, cw)\n",
        "\n",
        "print(\"\\n‚úì Experimento conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb97adcf",
      "metadata": {
        "id": "cb97adcf"
      },
      "source": [
        "## 8.2 Experimento: Sensibilidade ao Œ±\n",
        "\n",
        "```python\n",
        "# Testar diferentes valores de alpha\n",
        "alphas = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "results = {}\n",
        "\n",
        "print(\"Testando sensibilidade ao par√¢metro Œ±:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for alpha in alphas:\n",
        "    print(f\"\\n‚Üí Treinando com Œ± = {alpha}...\")\n",
        "    Q, rewards = q_learning(\n",
        "        gw,\n",
        "        n_episodes=1000,\n",
        "        alpha=alpha,\n",
        "        gamma=0.9,\n",
        "        epsilon=0.1\n",
        "    )\n",
        "    \n",
        "    results[f'Œ±={alpha}'] = rewards\n",
        "    \n",
        "    # Calcular valor m√©dio final\n",
        "    values = []\n",
        "    for state in gw.states:\n",
        "        if not gw.is_terminal(state) and state not in gw.walls:\n",
        "            state_idx = state[0] * gw.cols + state[1]\n",
        "            values.append(np.max(Q[state_idx]))\n",
        "    \n",
        "    avg_value = np.mean(values)\n",
        "    print(f\"   Valor m√©dio final: {avg_value:.4f}\")\n",
        "\n",
        "# Plotar compara√ß√£o\n",
        "plot_learning_curves(results, window=50,\n",
        "                    title=\"Sensibilidade ao par√¢metro Œ± (Q-Learning)\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536c0a78",
      "metadata": {
        "id": "536c0a78"
      },
      "outputs": [],
      "source": [
        "# Testar diferentes valores de alpha\n",
        "alphas = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93fc07a",
      "metadata": {
        "id": "f93fc07a"
      },
      "outputs": [],
      "source": [
        "print(\"Testando sensibilidade ao par√¢metro Œ±:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for alpha in alphas:\n",
        "    print(f\"\\n‚Üí Treinando com Œ± = {alpha}...\")\n",
        "    Q, rewards = q_learning(\n",
        "        gw,\n",
        "        n_episodes=1000,\n",
        "        alpha=alpha,\n",
        "        gamma=0.9,\n",
        "        epsilon=0.1\n",
        "    )\n",
        "\n",
        "    results[f'Œ±={alpha}'] = rewards\n",
        "\n",
        "    # Calcular valor m√©dio final\n",
        "    values = []\n",
        "    for state in gw.states:\n",
        "        if not gw.is_terminal(state) and state not in gw.walls:\n",
        "            state_idx = state[0] * gw.cols + state[1]\n",
        "            values.append(np.max(Q[state_idx]))\n",
        "\n",
        "    avg_value = np.mean(values)\n",
        "    print(f\"   Valor m√©dio final: {avg_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176b59ca",
      "metadata": {
        "id": "176b59ca"
      },
      "outputs": [],
      "source": [
        "# Plotar compara√ß√£o\n",
        "plot_learning_curves(results, window=100,\n",
        "                    title=\"Sensibilidade ao par√¢metro Œ± (Q-Learning)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed1bae6",
      "metadata": {
        "id": "bed1bae6"
      },
      "source": [
        "### 8.3 Experimento: TD vs Monte Carlo\n",
        "\n",
        "```python\n",
        "# Criar pol√≠tica de teste\n",
        "policy = create_fixed_policy(gw, 'L')\n",
        "\n",
        "# Treinar ambos\n",
        "print(\"‚Üí Treinando TD(0)...\")\n",
        "V_td = td_zero_prediction(gw, policy, n_episodes=1000, alpha=0.1, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Monte Carlo...\")\n",
        "V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000, alpha=0.1, verbose=True)\n",
        "\n",
        "# Visualizar lado a lado\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# TD(0)\n",
        "plot_value_heatmap(V_td, gw, title=\"TD(0) - Valores V(s)\")\n",
        "\n",
        "# Monte Carlo\n",
        "plot_value_heatmap(V_mc, gw, title=\"Monte Carlo - Valores V(s)\")\n",
        "\n",
        "# Comparar numericamente\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARA√á√ÉO TD(0) vs MONTE CARLO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Estado':<15} {'TD(0)':<15} {'MC':<15} {'|Diferen√ßa|':<15}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for state in gw.states:\n",
        "    if not gw.is_terminal(state) and state not in gw.walls:\n",
        "        diff = abs(V_td[state] - V_mc[state])\n",
        "        print(f\"{str(state):<15} {V_td[state]:<15.4f} {V_mc[state]:<15.4f} {diff:<15.6f}\")\n",
        "\n",
        "# Estat√≠sticas\n",
        "td_values = [V_td[s] for s in gw.states if not gw.is_terminal(s) and s not in gw.walls]\n",
        "mc_values = [V_mc[s] for s in gw.states if not gw.is_terminal(s) and s not in gw.walls]\n",
        "\n",
        "print(\"\\nESTAT√çSTICAS:\")\n",
        "print(f\"TD(0) - M√©dia: {np.mean(td_values):.4f}, Desvio: {np.std(td_values):.4f}\")\n",
        "print(f\"MC    - M√©dia: {np.mean(mc_values):.4f}, Desvio: {np.std(mc_values):.4f}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1568bd9c",
      "metadata": {
        "id": "1568bd9c"
      },
      "outputs": [],
      "source": [
        "# Criar pol√≠tica de teste\n",
        "policy = create_fixed_policy(gw, 'L')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113f9793",
      "metadata": {
        "id": "113f9793"
      },
      "outputs": [],
      "source": [
        "# Treinar ambos\n",
        "print(\"‚Üí Treinando TD(0)...\")\n",
        "V_td = td_zero_prediction(gw, policy, n_episodes=1000, alpha=0.1, verbose=True)\n",
        "\n",
        "print(\"\\n‚Üí Treinando Monte Carlo...\")\n",
        "V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000, alpha=0.1, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3dab5bc",
      "metadata": {
        "id": "d3dab5bc"
      },
      "outputs": [],
      "source": [
        "# TD(0)\n",
        "plot_value_heatmap(V_td, gw, title=\"TD(0) - Valores V(s)\")\n",
        "\n",
        "# Monte Carlo\n",
        "plot_value_heatmap(V_mc, gw, title=\"Monte Carlo - Valores V(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08f6443",
      "metadata": {
        "id": "f08f6443"
      },
      "outputs": [],
      "source": [
        "# Comparar numericamente\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARA√á√ÉO TD(0) vs MONTE CARLO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Estado':<15} {'TD(0)':<15} {'MC':<15} {'|Diferen√ßa|':<15}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for state in gw.states:\n",
        "    if not gw.is_terminal(state) and state not in gw.walls:\n",
        "        diff = abs(V_td[state] - V_mc[state])\n",
        "        print(f\"{str(state):<15} {V_td[state]:<15.4f} {V_mc[state]:<15.4f} {diff:<15.6f}\")\n",
        "\n",
        "# Estat√≠sticas\n",
        "td_values = [V_td[s] for s in gw.states if not gw.is_terminal(s) and s not in gw.walls]\n",
        "mc_values = [V_mc[s] for s in gw.states if not gw.is_terminal(s) and s not in gw.walls]\n",
        "\n",
        "print(\"\\nESTAT√çSTICAS:\")\n",
        "print(f\"TD(0) - M√©dia: {np.mean(td_values):.4f}, Desvio: {np.std(td_values):.4f}\")\n",
        "print(f\"MC    - M√©dia: {np.mean(mc_values):.4f}, Desvio: {np.std(mc_values):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35df7af9",
      "metadata": {
        "id": "35df7af9"
      },
      "source": [
        "### 8.4 Experimento: Grid Personalizado\n",
        "\n",
        "```python\n",
        "# Criar ambiente personalizado\n",
        "gw_custom = create_custom_gridworld(\n",
        "    rows=6,\n",
        "    cols=6,\n",
        "    walls=[(2, 2), (2, 3), (3, 2), (3, 3)],\n",
        "    terminals={(0, 5): 10.0, (5, 0): -10.0},\n",
        "    gamma=0.95,\n",
        "    noise=0.1,\n",
        "    living_reward=-0.1\n",
        ")\n",
        "\n",
        "print_gridworld_info(gw_custom)\n",
        "visualize_gridworld(gw_custom, title=\"Grid Personalizado 6x6\")\n",
        "\n",
        "# Treinar Q-Learning\n",
        "Q, rewards = q_learning(\n",
        "    gw_custom,\n",
        "    n_episodes=2000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.95,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Visualizar resultados\n",
        "policy = get_greedy_policy(Q, gw_custom)\n",
        "visualize_gridworld(gw_custom, policy=policy, title=\"Pol√≠tica Aprendida - Grid 6x6\")\n",
        "visualize_q_table_detailed(Q, gw_custom, title=\"Q-Values - Grid 6x6\")\n",
        "\n",
        "# Plotar curva de aprendizado\n",
        "plot_learning_curves({'Q-Learning': rewards}, window=100,\n",
        "                    title=\"Aprendizado no Grid 6x6\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a8b403",
      "metadata": {
        "id": "e6a8b403"
      },
      "outputs": [],
      "source": [
        "# Criar ambiente personalizado\n",
        "gw_custom = create_custom_gridworld(\n",
        "    rows=6,\n",
        "    cols=6,\n",
        "    walls=[(2, 2), (2, 3), (3, 2), (3, 3)],\n",
        "    terminals={(0, 5): 10.0, (5, 0): -10.0},\n",
        "    gamma=0.95,\n",
        "    noise=0.1,\n",
        "    living_reward=-0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90c5afcb",
      "metadata": {
        "id": "90c5afcb"
      },
      "outputs": [],
      "source": [
        "print_gridworld_info(gw_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acafdd8",
      "metadata": {
        "id": "3acafdd8"
      },
      "outputs": [],
      "source": [
        "visualize_gridworld(gw_custom, title=\"Grid Personalizado 6x6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc05d0a5",
      "metadata": {
        "id": "cc05d0a5"
      },
      "outputs": [],
      "source": [
        "# Treinar Q-Learning\n",
        "Q, rewards = q_learning(\n",
        "    gw_custom,\n",
        "    n_episodes=2000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.95,\n",
        "    epsilon=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c416ec96",
      "metadata": {
        "id": "c416ec96"
      },
      "outputs": [],
      "source": [
        "# Visualizar resultados\n",
        "policy = get_greedy_policy(Q, gw_custom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd71ebb",
      "metadata": {
        "id": "0bd71ebb"
      },
      "outputs": [],
      "source": [
        "visualize_gridworld(gw_custom, policy=policy, title=\"Pol√≠tica Aprendida - Grid 6x6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9ad64b",
      "metadata": {
        "id": "bf9ad64b"
      },
      "outputs": [],
      "source": [
        "visualize_q_table_detailed(Q, gw_custom, title=\"Q-Values - Grid 6x6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6327e5d",
      "metadata": {
        "id": "d6327e5d"
      },
      "outputs": [],
      "source": [
        "# Plotar curva de aprendizado\n",
        "plot_learning_curves({'Q-Learning': rewards}, window=100,\n",
        "                    title=\"Aprendizado no Grid 6x6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e8df5a6",
      "metadata": {
        "id": "0e8df5a6"
      },
      "source": [
        "## 9. Usando help()\n",
        "\n",
        "Todos os m√≥dulos, classes e fun√ß√µes possuem documenta√ß√£o completa que pode ser acessada com `help()`.\n",
        "\n",
        "### 9.1 Ajuda para M√≥dulos\n",
        "\n",
        "```python\n",
        "# Documenta√ß√£o do m√≥dulo inteiro\n",
        "help(environment)\n",
        "help(algorithms)\n",
        "help(visualization)\n",
        "```\n",
        "\n",
        "### 9.2 Ajuda para Classes\n",
        "\n",
        "```python\n",
        "from environment import GridWorld\n",
        "\n",
        "# Documenta√ß√£o da classe GridWorld\n",
        "help(GridWorld)\n",
        "\n",
        "# M√©todos espec√≠ficos\n",
        "help(GridWorld.sample_transition)\n",
        "help(GridWorld.set_terminal)\n",
        "```\n",
        "\n",
        "### 9.3 Ajuda para Fun√ß√µes\n",
        "\n",
        "```python\n",
        "from algorithms import sarsa, q_learning\n",
        "from visualization import plot_learning_curves\n",
        "\n",
        "# Documenta√ß√£o de algoritmos\n",
        "help(sarsa)\n",
        "help(q_learning)\n",
        "\n",
        "# Documenta√ß√£o de visualiza√ß√£o\n",
        "help(plot_learning_curves)\n",
        "help(visualize_q_values)\n",
        "```\n",
        "\n",
        "### 9.4 Exemplo de Uso do help()\n",
        "\n",
        "```python\n",
        "# Ver documenta√ß√£o do SARSA\n",
        "help(sarsa)\n",
        "```\n",
        "\n",
        "**Sa√≠da:**\n",
        "```\n",
        "Help on function sarsa in module algorithms:\n",
        "\n",
        "sarsa(gridworld: environment.GridWorld, n_episodes: int = 1000,\n",
        "      alpha: float = 0.1, gamma: float = 0.9, epsilon: float = 0.1,\n",
        "      q_init: float = 0.0, initial_state: Tuple[int, int] = None,\n",
        "      verbose: bool = False) -> Tuple[numpy.ndarray, List[float]]\n",
        "    \n",
        "    Algoritmo SARSA para controle on-policy.\n",
        "    \n",
        "    Aprende Q*(s,a) seguindo pol√≠tica Œµ-greedy.\n",
        "    Atualiza Q usando a a√ß√£o que ser√° realmente tomada.\n",
        "    \n",
        "    F√≥rmula:\n",
        "    --------\n",
        "    Q(St, At) ‚Üê Q(St, At) + Œ±[Rt+1 + Œ≥Q(St+1, At+1) - Q(St, At)]\n",
        "    \n",
        "    Par√¢metros:\n",
        "    -----------\n",
        "    gridworld : GridWorld\n",
        "        Ambiente\n",
        "    n_episodes : int, default=1000\n",
        "        N√∫mero de epis√≥dios\n",
        "    alpha : float, default=0.1\n",
        "        Taxa de aprendizado\n",
        "        - Valores t√≠picos: 0.01 a 0.5\n",
        "    ...\n",
        "```\n",
        "\n",
        "### 9.5 Listando Fun√ß√µes Dispon√≠veis\n",
        "\n",
        "```python\n",
        "# Ver todas as fun√ß√µes de um m√≥dulo\n",
        "import algorithms\n",
        "\n",
        "# Listar fun√ß√µes p√∫blicas (sem _ no in√≠cio)\n",
        "public_functions = [name for name in dir(algorithms) if not name.startswith('_')]\n",
        "print(\"Fun√ß√µes dispon√≠veis em algorithms:\")\n",
        "for func in public_functions:\n",
        "    print(f\"  - {func}\")\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b7054c",
      "metadata": {
        "id": "45b7054c"
      },
      "source": [
        "## 10. Dicas e Boas Pr√°ticas\n",
        "\n",
        "### 10.1 Salvando Resultados\n",
        "\n",
        "```python\n",
        "# Salvar tabela Q\n",
        "np.save('Q_sarsa.npy', Q_sarsa)\n",
        "\n",
        "# Carregar tabela Q\n",
        "Q_loaded = np.load('Q_sarsa.npy')\n",
        "\n",
        "# Salvar recompensas\n",
        "np.save('rewards_sarsa.npy', np.array(rewards_sarsa))\n",
        "```\n",
        "\n",
        "### 10.2 Organizando Experimentos\n",
        "\n",
        "```python\n",
        "# Criar fun√ß√£o para experimento completo\n",
        "def run_experiment(algorithm_name, algorithm_func, gridworld, params):\n",
        "    \"\"\"Executa experimento completo e salva resultados.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENTO: {algorithm_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Treinar\n",
        "    Q, rewards = algorithm_func(gridworld, **params, verbose=True)\n",
        "    \n",
        "    # Extrair pol√≠tica\n",
        "    policy = get_greedy_policy(Q, gridworld)\n",
        "    \n",
        "    # Visualizar\n",
        "    visualize_gridworld(gridworld, policy=policy,\n",
        "                       title=f\"{algorithm_name} - Pol√≠tica\")\n",
        "    visualize_q_values(Q, gridworld,\n",
        "                      title=f\"{algorithm_name} - Valores Q\")\n",
        "    \n",
        "    # Salvar\n",
        "    np.save(f'Q_{algorithm_name}.npy', Q)\n",
        "    np.save(f'rewards_{algorithm_name}.npy', np.array(rewards))\n",
        "    \n",
        "    print(f\"\\n‚úì {algorithm_name} conclu√≠do e salvo!\")\n",
        "    \n",
        "    return Q, rewards, policy\n",
        "\n",
        "# Usar\n",
        "params = {'n_episodes': 1000, 'alpha': 0.1, 'gamma': 0.9, 'epsilon': 0.1}\n",
        "Q_sarsa, rewards_sarsa, policy_sarsa = run_experiment(\n",
        "    'SARSA', sarsa, gw, params\n",
        ")\n",
        "```\n",
        "\n",
        "### 10.3 Executando M√∫ltiplas Runs\n",
        "\n",
        "```python\n",
        "# Para reduzir vari√¢ncia, execute m√∫ltiplas vezes\n",
        "def run_multiple_experiments(algorithm_func, gridworld, params, n_runs=10):\n",
        "    \"\"\"Executa algoritmo m√∫ltiplas vezes e retorna m√©dia.\"\"\"\n",
        "    all_rewards = []\n",
        "    \n",
        "    for run in range(n_runs):\n",
        "        print(f\"Run {run+1}/{n_runs}...\")\n",
        "        _, rewards = algorithm_func(gridworld, **params)\n",
        "        all_rewards.append(rewards)\n",
        "    \n",
        "    # Calcular m√©dia e desvio padr√£o\n",
        "    all_rewards = np.array(all_rewards)\n",
        "    mean_rewards = np.mean(all_rewards, axis=0)\n",
        "    std_rewards = np.std(all_rewards, axis=0)\n",
        "    \n",
        "    return mean_rewards, std_rewards\n",
        "\n",
        "# Usar\n",
        "mean_rewards, std_rewards = run_multiple_experiments(\n",
        "    q_learning, gw,\n",
        "    {'n_episodes': 500, 'alpha': 0.1, 'gamma': 0.9, 'epsilon': 0.1},\n",
        "    n_runs=10\n",
        ")\n",
        "\n",
        "# Plotar com intervalo de confian√ßa\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(mean_rewards, label='M√©dia')\n",
        "plt.fill_between(range(len(mean_rewards)),\n",
        "                 mean_rewards - std_rewards,\n",
        "                 mean_rewards + std_rewards,\n",
        "                 alpha=0.3, label='¬±1 Desvio Padr√£o')\n",
        "plt.xlabel('Epis√≥dio')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.title('Q-Learning - 10 Runs')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 10.4 Debug e An√°lise\n",
        "\n",
        "```python\n",
        "# Verificar se pol√≠tica faz sentido\n",
        "def analyze_policy(policy, gridworld):\n",
        "    \"\"\"Analisa uma pol√≠tica e mostra estat√≠sticas.\"\"\"\n",
        "    print(\"\\nAN√ÅLISE DA POL√çTICA:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Contar a√ß√µes\n",
        "    action_counts = {action: 0 for action in gridworld.actions}\n",
        "    for state, action in policy.items():\n",
        "        action_counts[action] += 1\n",
        "    \n",
        "    print(\"Distribui√ß√£o de a√ß√µes:\")\n",
        "    for action, count in action_counts.items():\n",
        "        percentage = (count / len(policy)) * 100\n",
        "        print(f\"  {action}: {count} estados ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Mostrar pol√≠tica por linha\n",
        "    print(\"\\nPol√≠tica por linha:\")\n",
        "    for row in range(gridworld.rows):\n",
        "        print(f\"  Linha {row}: \", end='')\n",
        "        for col in range(gridworld.cols):\n",
        "            state = (row, col)\n",
        "            if state in policy:\n",
        "                print(f\"{policy[state]}\", end=' ')\n",
        "            else:\n",
        "                print(\"¬∑\", end=' ')\n",
        "        print()\n",
        "\n",
        "# Usar\n",
        "analyze_policy(policy_sarsa, gw)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06dde34d",
      "metadata": {
        "id": "06dde34d"
      },
      "outputs": [],
      "source": [
        "# Criar fun√ß√£o para experimento completo\n",
        "def run_experiment(algorithm_name, algorithm_func, gridworld, params):\n",
        "    \"\"\"Executa experimento completo e salva resultados.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENTO: {algorithm_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Treinar\n",
        "    Q, rewards = algorithm_func(gridworld, **params, verbose=True)\n",
        "\n",
        "    # Extrair pol√≠tica\n",
        "    policy = get_greedy_policy(Q, gridworld)\n",
        "\n",
        "    # Visualizar\n",
        "    visualize_gridworld(gridworld, policy=policy,\n",
        "                       title=f\"{algorithm_name} - Pol√≠tica\")\n",
        "    visualize_q_values(Q, gridworld,\n",
        "                      title=f\"{algorithm_name} - Valores Q\")\n",
        "\n",
        "    # Salvar\n",
        "    np.save(f'Q_{algorithm_name}.npy', Q)\n",
        "    np.save(f'rewards_{algorithm_name}.npy', np.array(rewards))\n",
        "\n",
        "    print(f\"\\n‚úì {algorithm_name} conclu√≠do e salvo!\")\n",
        "\n",
        "    return Q, rewards, policy\n",
        "\n",
        "# Usar\n",
        "params = {'n_episodes': 1000, 'alpha': 0.1, 'gamma': 0.9, 'epsilon': 0.1}\n",
        "Q_sarsa, rewards_sarsa, policy_sarsa = run_experiment(\n",
        "    'SARSA', sarsa, gw, params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8729b7cd",
      "metadata": {
        "id": "8729b7cd"
      },
      "outputs": [],
      "source": [
        "# Para reduzir vari√¢ncia, execute m√∫ltiplas vezes\n",
        "def run_multiple_experiments(algorithm_func, gridworld, params, n_runs=10):\n",
        "    \"\"\"Executa algoritmo m√∫ltiplas vezes e retorna m√©dia.\"\"\"\n",
        "    all_rewards = []\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        print(f\"Run {run+1}/{n_runs}...\")\n",
        "        _, rewards = algorithm_func(gridworld, **params)\n",
        "        all_rewards.append(rewards)\n",
        "\n",
        "    # Calcular m√©dia e desvio padr√£o\n",
        "    all_rewards = np.array(all_rewards)\n",
        "    mean_rewards = np.mean(all_rewards, axis=0)\n",
        "    std_rewards = np.std(all_rewards, axis=0)\n",
        "\n",
        "    return mean_rewards, std_rewards\n",
        "\n",
        "# Usar\n",
        "mean_rewards, std_rewards = run_multiple_experiments(\n",
        "    q_learning, gw,\n",
        "    {'n_episodes': 500, 'alpha': 0.1, 'gamma': 0.9, 'epsilon': 0.1},\n",
        "    n_runs=10\n",
        ")\n",
        "\n",
        "# Plotar com intervalo de confian√ßa\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(mean_rewards, label='M√©dia')\n",
        "plt.fill_between(range(len(mean_rewards)),\n",
        "                 mean_rewards - std_rewards,\n",
        "                 mean_rewards + std_rewards,\n",
        "                 alpha=0.3, label='¬±1 Desvio Padr√£o')\n",
        "plt.xlabel('Epis√≥dio')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.title('Q-Learning - 10 Runs')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef805dda",
      "metadata": {
        "id": "ef805dda"
      },
      "source": [
        "## 11. Troubleshooting\n",
        "\n",
        "### Problema: ImportError\n",
        "\n",
        "```python\n",
        "# Erro: ModuleNotFoundError: No module named 'environment'\n",
        "\n",
        "# Solu√ß√£o 1: Verificar se arquivos est√£o na mesma pasta\n",
        "import os\n",
        "print(\"Arquivos na pasta:\", os.listdir('.'))\n",
        "\n",
        "# Solu√ß√£o 2: Adicionar caminho manualmente\n",
        "import sys\n",
        "sys.path.append('/caminho/para/pasta/dos/modulos')\n",
        "\n",
        "# Solu√ß√£o 3: Usar importa√ß√£o relativa se em um pacote\n",
        "from .environment import GridWorld\n",
        "```\n",
        "\n",
        "### Problema: Valores n√£o convergem\n",
        "\n",
        "```python\n",
        "# Poss√≠veis causas e solu√ß√µes:\n",
        "\n",
        "# 1. Œ± muito alto ‚Üí diminuir alpha\n",
        "Q, _ = sarsa(gw, alpha=0.01)  # Em vez de 0.5\n",
        "\n",
        "# 2. Œµ muito alto ‚Üí diminuir epsilon\n",
        "Q, _ = sarsa(gw, epsilon=0.05)  # Em vez de 0.3\n",
        "\n",
        "# 3. Poucos epis√≥dios ‚Üí aumentar n_episodes\n",
        "Q, _ = sarsa(gw, n_episodes=5000)  # Em vez de 500\n",
        "\n",
        "# 4. Ambiente muito dif√≠cil ‚Üí ajustar living_reward\n",
        "gw.living_reward = -0.01  # Em vez de -0.1\n",
        "```\n",
        "\n",
        "### Problema: Gr√°ficos n√£o aparecem\n",
        "\n",
        "```python\n",
        "# Em notebooks Jupyter, adicionar:\n",
        "%matplotlib inline\n",
        "\n",
        "# Ou para gr√°ficos interativos:\n",
        "%matplotlib notebook\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c9bc6d",
      "metadata": {
        "id": "a5c9bc6d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}