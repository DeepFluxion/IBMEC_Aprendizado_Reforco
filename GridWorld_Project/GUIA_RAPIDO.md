# Guia R√°pido de Refer√™ncia - RL GridWorld

## üì¶ Arquivos do Projeto

```
‚îú‚îÄ‚îÄ environment.py          # Ambientes GridWorld
‚îú‚îÄ‚îÄ algorithms.py           # Algoritmos de RL
‚îú‚îÄ‚îÄ visualization.py        # Visualiza√ß√£o
‚îú‚îÄ‚îÄ TUTORIAL.md            # Tutorial completo (LEIA PRIMEIRO!)
‚îú‚îÄ‚îÄ README.md              # Documenta√ß√£o geral
‚îú‚îÄ‚îÄ EXEMPLOS_NOTEBOOK.md   # C√©lulas prontas para copiar
‚îî‚îÄ‚îÄ GUIA_RAPIDO.md         # Este arquivo
```

---

## ‚ö° Quick Start (3 minutos)

```python
# 1. Importar
from environment import create_classic_gridworld
from algorithms import q_learning, get_greedy_policy
from visualization import visualize_gridworld

# 2. Criar ambiente
gw = create_classic_gridworld()

# 3. Treinar
Q, rewards = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# 4. Visualizar
policy = get_greedy_policy(Q, gw)
visualize_gridworld(gw, policy=policy, title="Pol√≠tica")
```

---

## üìñ Documenta√ß√£o R√°pida

### Ver ajuda de qualquer fun√ß√£o:
```python
help(q_learning)  # Mostra documenta√ß√£o completa
```

### Listar fun√ß√µes dispon√≠veis:
```python
import algorithms
print([f for f in dir(algorithms) if not f.startswith('_')])
```

---

## üèóÔ∏è Criar Ambientes

### Grid Cl√°ssico 4x3:
```python
gw = create_classic_gridworld()
```

### Grid Personalizado:
```python
gw = create_custom_gridworld(
    rows=5, cols=5,
    walls=[(1,1), (2,2)],
    terminals={(0,4): 10.0},
    gamma=0.95
)
```

### Cliff World:
```python
gw = create_cliff_world()
```

---

## üß† Algoritmos

### Predi√ß√£o (Avalia√ß√£o de Pol√≠tica):
```python
# TD(0)
V = td_zero_prediction(gw, policy, n_episodes=1000, alpha=0.1)

# Monte Carlo
V = first_visit_mc_prediction(gw, policy, n_episodes=1000, alpha=0.1)
```

### Controle (Busca de Pol√≠tica √ìtima):
```python
# SARSA (on-policy)
Q, rewards = sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# Q-Learning (off-policy)
Q, rewards = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# Expected SARSA
Q, rewards = expected_sarsa(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# Monte Carlo Control
Q, rewards = first_visit_mc_control(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# MC Exploring Starts
Q, rewards = mc_exploring_starts(gw, n_episodes=1000, alpha=0.1, max_steps=500)
```

### Extrair Pol√≠tica:
```python
policy = get_greedy_policy(Q, gw)
```

---

## üìä Visualiza√ß√£o

### Grid com Pol√≠tica/Valores:
```python
visualize_gridworld(gw, values=V, policy=policy, title="Meu Grid")
```

### Q-Values:
```python
# Valores m√°ximos
visualize_q_values(Q, gw, title="Max Q-Values")

# Detalhado (todas as a√ß√µes)
visualize_q_table_detailed(Q, gw, title="Q-Values Detalhados")

# Imprimir no console
print_q_table(Q, gw, title="Tabela Q")
```

### Curvas de Aprendizado:
```python
plot_learning_curves({
    'SARSA': rewards_sarsa,
    'Q-Learning': rewards_qlearning
}, window=100)
```

### Heatmaps:
```python
# Valores V(s)
plot_value_heatmap(V, gw, title="Heatmap de Valores")

# Q-values
plot_q_value_heatmap(Q, gw, title="Heatmap Q")
plot_q_value_heatmap(Q, gw, action='N', title="Q(s, Norte)")
```

### Comparar Algoritmos:
```python
compare_algorithms({
    'SARSA': Q_sarsa,
    'Q-Learning': Q_qlearning
}, gw)
```

---

## üéõÔ∏è Par√¢metros Comuns

### alpha (Taxa de Aprendizado)
- **0.01-0.05**: Lento mas est√°vel
- **0.1**: Padr√£o, bom equil√≠brio
- **0.3-0.5**: R√°pido mas pode oscilar

### epsilon (Explora√ß√£o)
- **0.0**: Sem explora√ß√£o (s√≥ exploitation)
- **0.1**: Padr√£o, 10% explora√ß√£o
- **0.2-0.3**: Mais explora√ß√£o

### gamma (Desconto)
- **0.9**: Padr√£o, valoriza futuro pr√≥ximo
- **0.95-0.99**: Mais vision√°rio
- **0.5-0.7**: Mais m√≠ope (imediato)

### n_episodes
- **500-1000**: R√°pido, teste inicial
- **1000-2000**: Padr√£o
- **5000+**: Converg√™ncia garantida

---

## üîß Padr√µes de Uso

### Experimento B√°sico:
```python
# 1. Ambiente
gw = create_classic_gridworld()

# 2. Treinar
Q, rewards = q_learning(gw, n_episodes=1000, alpha=0.1, epsilon=0.1)

# 3. Visualizar
policy = get_greedy_policy(Q, gw)
visualize_gridworld(gw, policy=policy)
plot_learning_curves({'Q-Learning': rewards})
```

### Comparar 2 Algoritmos:
```python
# Treinar
Q1, r1 = sarsa(gw, n_episodes=1000)
Q2, r2 = q_learning(gw, n_episodes=1000)

# Comparar
plot_learning_curves({'SARSA': r1, 'Q-Learning': r2})
compare_algorithms({'SARSA': Q1, 'Q-Learning': Q2}, gw)
```

### An√°lise de Sensibilidade:
```python
alphas = [0.01, 0.1, 0.5]
results = {}

for alpha in alphas:
    Q, rewards = q_learning(gw, alpha=alpha)
    results[f'Œ±={alpha}'] = rewards

plot_learning_curves(results)
```

---

## üíæ Salvar/Carregar

```python
# Salvar
np.save('Q_table.npy', Q)
np.save('rewards.npy', rewards)

# Carregar
Q = np.load('Q_table.npy')
rewards = np.load('rewards.npy')
```

---

## üêõ Problemas Comuns

### M√≥dulos n√£o encontrados:
```python
# Verificar arquivos na pasta
import os
print(os.listdir('.'))
# Deve mostrar: environment.py, algorithms.py, visualization.py
```

### Valores n√£o convergem:
```python
# Solu√ß√£o 1: Mais epis√≥dios
Q, _ = q_learning(gw, n_episodes=5000)

# Solu√ß√£o 2: Alpha menor
Q, _ = q_learning(gw, alpha=0.05)

# Solu√ß√£o 3: Mais explora√ß√£o
Q, _ = q_learning(gw, epsilon=0.2)
```

### Gr√°ficos n√£o aparecem:
```python
# Adicionar no in√≠cio do notebook
%matplotlib inline
```

---

## üìö Onde Procurar

- **Como criar ambientes?** ‚Üí `TUTORIAL.md` Se√ß√£o 3
- **Como usar algoritmos?** ‚Üí `TUTORIAL.md` Se√ß√µes 5-6
- **Como visualizar?** ‚Üí `TUTORIAL.md` Se√ß√£o 7
- **Exemplos completos?** ‚Üí `TUTORIAL.md` Se√ß√£o 8
- **C√©lulas prontas?** ‚Üí `EXEMPLOS_NOTEBOOK.md`
- **Documenta√ß√£o de fun√ß√£o?** ‚Üí `help(nome_funcao)`

---

## üéØ Checklist para Experimento

- [ ] Importar m√≥dulos
- [ ] Criar/carregar ambiente
- [ ] Visualizar ambiente
- [ ] Definir par√¢metros
- [ ] Treinar algoritmo(s)
- [ ] Extrair pol√≠tica
- [ ] Visualizar resultados
- [ ] Analisar valores
- [ ] Salvar resultados (opcional)

---

## üöÄ Pr√≥ximos Passos

1. Execute o Quick Start acima
2. Leia `TUTORIAL.md` por completo
3. Copie c√©lulas de `EXEMPLOS_NOTEBOOK.md`
4. Experimente diferentes par√¢metros
5. Crie seus pr√≥prios ambientes
6. Compare algoritmos

---

## üìû Ajuda

```python
# Ver documenta√ß√£o
help(algorithms)
help(environment)
help(visualization)

# Ver par√¢metros de fun√ß√£o
help(q_learning)
help(create_custom_gridworld)
```

---

**Criado para fins educacionais - Aprendizado por Refor√ßo**
