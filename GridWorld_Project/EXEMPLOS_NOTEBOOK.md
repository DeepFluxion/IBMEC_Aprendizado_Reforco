# Exemplos de C√≥digo - Para Copiar e Colar no Jupyter Notebook

## üöÄ C√âLULA 1: Imports Iniciais

```python
# Imports b√°sicos
import numpy as np
import matplotlib.pyplot as plt

# Configura√ß√£o para notebooks
%matplotlib inline

# Importar m√≥dulos de RL
from environment import (
    GridWorld,
    create_classic_gridworld,
    create_custom_gridworld,
    create_cliff_world,
    print_gridworld_info
)

from algorithms import (
    # Predi√ß√£o
    td_zero_prediction,
    first_visit_mc_prediction,
    # Controle
    sarsa,
    q_learning,
    expected_sarsa,
    first_visit_mc_control,
    mc_exploring_starts,
    # Auxiliares
    get_greedy_policy
)

from visualization import (
    visualize_gridworld,
    visualize_q_values,
    visualize_q_table_detailed,
    plot_learning_curves,
    plot_value_evolution,
    plot_value_heatmap,
    plot_q_value_heatmap,
    compare_algorithms,
    print_q_table
)

print("‚úì M√≥dulos importados com sucesso!")
```

---

## üèóÔ∏è C√âLULA 2: Criar Ambiente Cl√°ssico

```python
# Criar GridWorld 4x3 cl√°ssico
gw = create_classic_gridworld()

# Visualizar
visualize_gridworld(gw, title="GridWorld 4x3 Cl√°ssico")

# Informa√ß√µes
print_gridworld_info(gw)
```

---

## üéØ C√âLULA 3: Experimento R√°pido - Q-Learning

```python
# Treinar Q-Learning
print("‚Üí Treinando Q-Learning...")
Q, rewards = q_learning(
    gridworld=gw,
    n_episodes=1000,
    alpha=0.1,
    gamma=0.9,
    epsilon=0.1,
    verbose=True
)

# Extrair pol√≠tica
policy = get_greedy_policy(Q, gw)

# Visualizar
visualize_gridworld(gw, policy=policy, title="Q-Learning - Pol√≠tica Aprendida")
visualize_q_values(Q, gw, title="Q-Learning - Valores Q")

print("\n‚úì Treinamento conclu√≠do!")
```

---

## üìä C√âLULA 4: Comparar SARSA vs Q-Learning vs Expected SARSA

```python
# Par√¢metros
PARAMS = {
    'n_episodes': 1000,
    'alpha': 0.1,
    'gamma': 0.9,
    'epsilon': 0.1
}

# Treinar os tr√™s algoritmos
print("‚Üí Treinando SARSA...")
Q_sarsa, rewards_sarsa = sarsa(gw, **PARAMS, verbose=True)

print("\n‚Üí Treinando Q-Learning...")
Q_qlearning, rewards_qlearning = q_learning(gw, **PARAMS, verbose=True)

print("\n‚Üí Treinando Expected SARSA...")
Q_expected, rewards_expected = expected_sarsa(gw, **PARAMS, verbose=True)

# Visualizar curvas de aprendizado
plot_learning_curves({
    'SARSA': rewards_sarsa,
    'Q-Learning': rewards_qlearning,
    'Expected SARSA': rewards_expected
}, window=100, title="Compara√ß√£o de Algoritmos TD")

# Comparar valores
compare_algorithms({
    'SARSA': Q_sarsa,
    'Q-Learning': Q_qlearning,
    'Expected SARSA': Q_expected
}, gw)

print("\n‚úì Compara√ß√£o conclu√≠da!")
```

---

## üé® C√âLULA 5: Visualiza√ß√µes Detalhadas

```python
# Visualizar pol√≠ticas aprendidas
for name, Q in [('SARSA', Q_sarsa), 
                ('Q-Learning', Q_qlearning),
                ('Expected SARSA', Q_expected)]:
    
    policy = get_greedy_policy(Q, gw)
    
    print(f"\n{'='*60}")
    print(f"{name}")
    print('='*60)
    
    # Pol√≠tica
    visualize_gridworld(gw, policy=policy, title=f"{name} - Pol√≠tica")
    
    # Valores Q
    visualize_q_values(Q, gw, title=f"{name} - Valores Q")
    
    # Q-values detalhados
    visualize_q_table_detailed(Q, gw, title=f"{name} - Q-Values Detalhados")
    
    # Heatmap
    plot_q_value_heatmap(Q, gw, title=f"{name} - Heatmap")
```

---

## üî¨ C√âLULA 6: Avalia√ß√£o de Pol√≠tica com TD(0)

```python
# Criar pol√≠tica de teste (sempre ir para Leste)
policy_test = {s: 'L' for s in gw.states if not gw.is_terminal(s)}

# Visualizar pol√≠tica
visualize_gridworld(gw, policy=policy_test, title="Pol√≠tica de Teste: Sempre Leste")

# Avaliar com TD(0)
print("\n‚Üí Avaliando pol√≠tica com TD(0)...")
V_td = td_zero_prediction(
    gridworld=gw,
    policy=policy_test,
    n_episodes=1000,
    alpha=0.1,
    verbose=True
)

# Visualizar resultados
visualize_gridworld(gw, values=V_td, policy=policy_test, 
                   title="TD(0) - Valores + Pol√≠tica")
plot_value_heatmap(V_td, gw, title="TD(0) - Heatmap de Valores")

print("\n‚úì Avalia√ß√£o conclu√≠da!")
```

---

## üÜö C√âLULA 7: TD(0) vs Monte Carlo

```python
# Criar pol√≠tica
policy = {s: 'N' for s in gw.states if not gw.is_terminal(s)}

# Treinar TD(0)
print("‚Üí Treinando TD(0)...")
V_td = td_zero_prediction(gw, policy, n_episodes=1000, alpha=0.1, verbose=True)

# Treinar Monte Carlo
print("\n‚Üí Treinando Monte Carlo...")
V_mc = first_visit_mc_prediction(gw, policy, n_episodes=1000, alpha=0.1, verbose=True)

# Comparar visualmente
print("\n‚Üí Visualizando resultados...")
visualize_gridworld(gw, values=V_td, title="TD(0) - Valores V(s)")
visualize_gridworld(gw, values=V_mc, title="Monte Carlo - Valores V(s)")

# Comparar numericamente
print("\n" + "="*60)
print("COMPARA√á√ÉO TD(0) vs MONTE CARLO")
print("="*60)
print(f"{'Estado':<15} {'TD(0)':<15} {'MC':<15} {'Diferen√ßa':<15}")
print("-"*60)

for state in gw.states:
    if not gw.is_terminal(state) and state not in gw.walls:
        diff = abs(V_td[state] - V_mc[state])
        print(f"{str(state):<15} {V_td[state]:<15.4f} {V_mc[state]:<15.4f} {diff:<15.6f}")

print("\n‚úì Compara√ß√£o conclu√≠da!")
```

---

## üîß C√âLULA 8: An√°lise de Sensibilidade - Alpha

```python
# Testar diferentes valores de alpha
alphas = [0.01, 0.05, 0.1, 0.3, 0.5]
results_alpha = {}

print("AN√ÅLISE DE SENSIBILIDADE - PAR√ÇMETRO ALPHA")
print("="*60)

for alpha in alphas:
    print(f"\n‚Üí Treinando com Œ± = {alpha}...")
    Q, rewards = q_learning(
        gw,
        n_episodes=500,
        alpha=alpha,
        gamma=0.9,
        epsilon=0.1
    )
    
    results_alpha[f'Œ±={alpha}'] = rewards
    
    # Calcular valor m√©dio
    values = []
    for state in gw.states:
        if not gw.is_terminal(state) and state not in gw.walls:
            state_idx = state[0] * gw.cols + state[1]
            values.append(np.max(Q[state_idx]))
    
    avg_value = np.mean(values)
    print(f"   Valor m√©dio final: {avg_value:.4f}")

# Plotar compara√ß√£o
plot_learning_curves(results_alpha, window=50,
                    title="Sensibilidade ao Œ± (Taxa de Aprendizado)")

print("\n‚úì An√°lise conclu√≠da!")
```

---

## üîß C√âLULA 9: An√°lise de Sensibilidade - Epsilon

```python
# Testar diferentes valores de epsilon
epsilons = [0.0, 0.05, 0.1, 0.2, 0.3]
results_epsilon = {}

print("AN√ÅLISE DE SENSIBILIDADE - PAR√ÇMETRO EPSILON")
print("="*60)

for eps in epsilons:
    print(f"\n‚Üí Treinando com Œµ = {eps}...")
    Q, rewards = q_learning(
        gw,
        n_episodes=500,
        alpha=0.1,
        gamma=0.9,
        epsilon=eps
    )
    
    results_epsilon[f'Œµ={eps}'] = rewards
    
    # Calcular recompensa m√©dia final
    avg_reward_final = np.mean(rewards[-100:])
    print(f"   Recompensa m√©dia (√∫ltimos 100): {avg_reward_final:.4f}")

# Plotar compara√ß√£o
plot_learning_curves(results_epsilon, window=50,
                    title="Sensibilidade ao Œµ (Explora√ß√£o)")

print("\n‚úì An√°lise conclu√≠da!")
```

---

## üèóÔ∏è C√âLULA 10: Grid Personalizado

```python
# Criar grid personalizado 6x6
gw_custom = create_custom_gridworld(
    rows=6,
    cols=6,
    walls=[(2, 2), (2, 3), (3, 2), (3, 3)],  # Bloco central
    terminals={(0, 5): 10.0, (5, 0): -10.0},  # Dois terminais
    gamma=0.95,
    noise=0.1,
    living_reward=-0.1
)

# Visualizar ambiente
visualize_gridworld(gw_custom, title="Grid Personalizado 6x6")
print_gridworld_info(gw_custom)

# Treinar
print("\n‚Üí Treinando Q-Learning no grid customizado...")
Q_custom, rewards_custom = q_learning(
    gw_custom,
    n_episodes=2000,
    alpha=0.1,
    gamma=0.95,
    epsilon=0.1,
    verbose=True
)

# Visualizar resultados
policy_custom = get_greedy_policy(Q_custom, gw_custom)
visualize_gridworld(gw_custom, policy=policy_custom, 
                   title="Grid 6x6 - Pol√≠tica Aprendida")
visualize_q_table_detailed(Q_custom, gw_custom, 
                          title="Grid 6x6 - Q-Values Detalhados")

# Curva de aprendizado
plot_learning_curves({'Q-Learning (6x6)': rewards_custom}, 
                    window=100,
                    title="Aprendizado no Grid 6x6")

print("\n‚úì Experimento no grid customizado conclu√≠do!")
```

---

## üéì C√âLULA 11: Verificar Documenta√ß√£o (help)

```python
# Ver documenta√ß√£o de um algoritmo
print("="*60)
print("DOCUMENTA√á√ÉO DO SARSA")
print("="*60)
help(sarsa)
```

---

## üíæ C√âLULA 12: Salvar e Carregar Resultados

```python
# Salvar resultados
print("‚Üí Salvando resultados...")

np.save('Q_sarsa.npy', Q_sarsa)
np.save('Q_qlearning.npy', Q_qlearning)
np.save('rewards_sarsa.npy', np.array(rewards_sarsa))
np.save('rewards_qlearning.npy', np.array(rewards_qlearning))

print("‚úì Resultados salvos!")

# Carregar resultados (em outra sess√£o)
print("\n‚Üí Carregando resultados...")

Q_sarsa_loaded = np.load('Q_sarsa.npy')
Q_qlearning_loaded = np.load('Q_qlearning.npy')
rewards_sarsa_loaded = np.load('rewards_sarsa.npy')
rewards_qlearning_loaded = np.load('rewards_qlearning.npy')

print("‚úì Resultados carregados!")

# Verificar
print(f"\nShape de Q_sarsa: {Q_sarsa_loaded.shape}")
print(f"N√∫mero de epis√≥dios: {len(rewards_sarsa_loaded)}")
```

---

## üìù C√âLULA 13: Imprimir Tabela Q Formatada

```python
# Imprimir tabela Q de forma leg√≠vel
print_q_table(Q_qlearning, gw, title="Q-LEARNING - TABELA Q COMPLETA")
```

---

## üéØ C√âLULA 14: Monte Carlo Exploring Starts

```python
# Treinar MC Exploring Starts
print("‚Üí Treinando MC Exploring Starts...")
Q_es, rewards_es = mc_exploring_starts(
    gridworld=gw,
    n_episodes=1000,
    alpha=0.1,
    max_steps=500,  # Limite para evitar loops infinitos
    verbose=True
)

# Extrair pol√≠tica
policy_es = get_greedy_policy(Q_es, gw)

# Visualizar
visualize_gridworld(gw, policy=policy_es, 
                   title="MC Exploring Starts - Pol√≠tica")
visualize_q_values(Q_es, gw, 
                  title="MC Exploring Starts - Valores Q")

# Comparar com Q-Learning
plot_learning_curves({
    'MC Exploring Starts': rewards_es,
    'Q-Learning': rewards_qlearning
}, window=100, title="MC Exploring Starts vs Q-Learning")

print("\n‚úì Treinamento conclu√≠do!")
```

---

## üî¨ C√âLULA 15: Experimento Completo - M√∫ltiplas Runs

```python
# Executar Q-Learning m√∫ltiplas vezes para reduzir vari√¢ncia
print("EXPERIMENTO: M√öLTIPLAS RUNS (10x)")
print("="*60)

n_runs = 10
all_rewards = []

for run in range(n_runs):
    print(f"\n‚Üí Run {run+1}/{n_runs}...")
    Q, rewards = q_learning(
        gw,
        n_episodes=500,
        alpha=0.1,
        gamma=0.9,
        epsilon=0.1
    )
    all_rewards.append(rewards)
    print(f"   Recompensa m√©dia final: {np.mean(rewards[-50:]):.2f}")

# Calcular m√©dia e desvio padr√£o
all_rewards = np.array(all_rewards)
mean_rewards = np.mean(all_rewards, axis=0)
std_rewards = np.std(all_rewards, axis=0)

# Plotar com intervalo de confian√ßa
plt.figure(figsize=(12, 6))
plt.plot(mean_rewards, label='M√©dia', linewidth=2)
plt.fill_between(range(len(mean_rewards)),
                 mean_rewards - std_rewards,
                 mean_rewards + std_rewards,
                 alpha=0.3, label='¬±1 Desvio Padr√£o')
plt.xlabel('Epis√≥dio')
plt.ylabel('Recompensa')
plt.title('Q-Learning - 10 Runs com Intervalo de Confian√ßa')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\n‚úì Experimento de m√∫ltiplas runs conclu√≠do!")
```

---

## üåä C√âLULA 16: Cliff World

```python
# Criar Cliff World
gw_cliff = create_cliff_world()

# Visualizar ambiente
visualize_gridworld(gw_cliff, title="Cliff World")
print_gridworld_info(gw_cliff)

# Treinar SARSA (conservador)
print("\n‚Üí Treinando SARSA (on-policy, conservador)...")
Q_sarsa_cliff, rewards_sarsa_cliff = sarsa(
    gw_cliff,
    n_episodes=500,
    alpha=0.1,
    gamma=0.9,
    epsilon=0.1
)

# Treinar Q-Learning (agressivo)
print("\n‚Üí Treinando Q-Learning (off-policy, agressivo)...")
Q_qlearn_cliff, rewards_qlearn_cliff = q_learning(
    gw_cliff,
    n_episodes=500,
    alpha=0.1,
    gamma=0.9,
    epsilon=0.1
)

# Comparar pol√≠ticas
policy_sarsa_cliff = get_greedy_policy(Q_sarsa_cliff, gw_cliff)
policy_qlearn_cliff = get_greedy_policy(Q_qlearn_cliff, gw_cliff)

visualize_gridworld(gw_cliff, policy=policy_sarsa_cliff,
                   title="Cliff World - SARSA (Conservador)")
visualize_gridworld(gw_cliff, policy=policy_qlearn_cliff,
                   title="Cliff World - Q-Learning (Agressivo)")

# Comparar curvas
plot_learning_curves({
    'SARSA (Conservador)': rewards_sarsa_cliff,
    'Q-Learning (Agressivo)': rewards_qlearn_cliff
}, window=50, title="Cliff World: SARSA vs Q-Learning")

print("\n‚úì Experimento Cliff World conclu√≠do!")
print("\nObserva√ß√£o: SARSA tende a ser mais conservador (evita o cliff)")
print("Q-Learning tende a ser mais agressivo (aprende caminho √≥timo mas arriscado)")
```

---

## üé® C√âLULA 17: Heatmaps de Q-Values por A√ß√£o

```python
# Criar heatmaps para cada a√ß√£o
acoes = ['N', 'S', 'L', 'O']
nomes = ['Norte', 'Sul', 'Leste', 'Oeste']

print("HEATMAPS DE Q-VALUES POR A√á√ÉO")
print("="*60)

for acao, nome in zip(acoes, nomes):
    plot_q_value_heatmap(Q_qlearning, gw, action=acao,
                        title=f"Q(s, {nome})")
```

---

## üìä C√âLULA 18: An√°lise Final Detalhada

```python
# An√°lise completa dos resultados
print("\n" + "="*70)
print("AN√ÅLISE FINAL - Q-LEARNING")
print("="*70)

# Estat√≠sticas da tabela Q
print("\n1. ESTAT√çSTICAS DA TABELA Q:")
print("-"*70)
print(f"   Valor m√°ximo:  {np.max(Q_qlearning):.4f}")
print(f"   Valor m√≠nimo:  {np.min(Q_qlearning):.4f}")
print(f"   Valor m√©dio:   {np.mean(Q_qlearning):.4f}")
print(f"   Desvio padr√£o: {np.std(Q_qlearning):.4f}")

# An√°lise da pol√≠tica
print("\n2. AN√ÅLISE DA POL√çTICA:")
print("-"*70)

policy_final = get_greedy_policy(Q_qlearning, gw)

action_counts = {'N': 0, 'S': 0, 'L': 0, 'O': 0}
for action in policy_final.values():
    action_counts[action] += 1

print("   Distribui√ß√£o de a√ß√µes:")
for action, count in action_counts.items():
    percentage = (count / len(policy_final)) * 100
    print(f"     {action}: {count} estados ({percentage:.1f}%)")

# An√°lise de converg√™ncia
print("\n3. AN√ÅLISE DE CONVERG√äNCIA:")
print("-"*70)
print(f"   Recompensa inicial (m√©dia primeiros 50): {np.mean(rewards_qlearning[:50]):.2f}")
print(f"   Recompensa final (m√©dia √∫ltimos 50):     {np.mean(rewards_qlearning[-50:]):.2f}")
print(f"   Melhoria:                                {np.mean(rewards_qlearning[-50:]) - np.mean(rewards_qlearning[:50]):.2f}")

# Visualizar pol√≠tica final
print("\n4. POL√çTICA FINAL:")
print("-"*70)
visualize_gridworld(gw, policy=policy_final, 
                   title="Q-Learning - Pol√≠tica Final")

print("\n‚úì An√°lise completa!")
```

---

## üîÑ C√âLULA 19: Template para Seu Pr√≥prio Experimento

```python
# ============================================
# SEU EXPERIMENTO PERSONALIZADO
# ============================================

# 1. CONFIGURA√á√ÉO
print("="*60)
print("MEU EXPERIMENTO CUSTOMIZADO")
print("="*60)

# Criar ambiente (escolha um)
# gw = create_classic_gridworld()
# gw = create_custom_gridworld(rows=5, cols=5, ...)
# gw = create_cliff_world()

# 2. VISUALIZAR AMBIENTE
# visualize_gridworld(gw, title="Meu Ambiente")
# print_gridworld_info(gw)

# 3. DEFINIR PAR√ÇMETROS
params = {
    'n_episodes': 1000,
    'alpha': 0.1,
    'gamma': 0.9,
    'epsilon': 0.1
}

# 4. TREINAR ALGORITMO (escolha um)
# Q, rewards = sarsa(gw, **params, verbose=True)
# Q, rewards = q_learning(gw, **params, verbose=True)
# Q, rewards = expected_sarsa(gw, **params, verbose=True)

# 5. EXTRAIR POL√çTICA
# policy = get_greedy_policy(Q, gw)

# 6. VISUALIZAR RESULTADOS
# visualize_gridworld(gw, policy=policy, title="Pol√≠tica Aprendida")
# visualize_q_values(Q, gw, title="Valores Q")
# plot_learning_curves({'Meu Algoritmo': rewards})

# 7. AN√ÅLISE
# print_q_table(Q, gw, title="Tabela Q Final")

print("\n‚úì Complete as se√ß√µes acima para seu experimento!")
```

---

## üìö C√âLULA 20: Links para Documenta√ß√£o

```python
print("="*70)
print("RECURSOS DE AJUDA")
print("="*70)

print("\n1. Ver todas as fun√ß√µes dispon√≠veis:")
print("   import algorithms")
print("   print([f for f in dir(algorithms) if not f.startswith('_')])")

print("\n2. Ver documenta√ß√£o de uma fun√ß√£o:")
print("   help(sarsa)")
print("   help(visualize_gridworld)")
print("   help(GridWorld)")

print("\n3. Ver par√¢metros de uma fun√ß√£o:")
print("   from inspect import signature")
print("   print(signature(q_learning))")

print("\n4. Tutorial completo:")
print("   Leia o arquivo TUTORIAL.md")

print("\n5. Exemplos r√°pidos:")
print("   Leia o arquivo README.md")

print("\n" + "="*70)
```

---

## ‚úÖ Pronto!

Copie e cole estas c√©lulas no seu Jupyter Notebook para come√ßar a experimentar!

Cada c√©lula √© independente e pode ser executada separadamente.

**Dica:** Execute as c√©lulas na ordem para um tutorial completo, ou escolha
c√©lulas espec√≠ficas para experimentos focados.
